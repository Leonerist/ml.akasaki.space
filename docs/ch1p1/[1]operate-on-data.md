# 数据操作

```python
import torch
print(torch.__version__)
```

    1.7.0+cu110

在深度学习中，我们通常会频繁地对数据进行操作。作为动手学深度学习的基础，本节将介绍如何对内存中的数据进行操作。

在Pytorch中，`tensor`是一个类，也是存储和变换数据的主要工具。如果你之前用过NumPy，你会发现`tensor`和NumPy的多维数组非常类似。然而，`tensor`提供GPU计算和自动求梯度等更多功能，这些使`tensor`更加适合深度学习。

## 创建 tensor

我们先介绍`tensor`的最基本功能，我们用arange函数创建一个行向量。

```python
x = torch.tensor(range(12))

print(x.shape)
```

    torch.Size([12])

```python
print(x)
```


    tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])

这时返回了一个`tensor`实例，其中包含了从0开始的12个连续整数。

我们可以通过`shape`属性来获取`tensor`实例的形状。

```python
print(x.shape)
```


    torch.Size([12])

我们也能够通过`len`得到`tensor`实例中元素（element）的总数。


```python
print(len(x))
```


    12

下面使用`reshape`函数把行向量`x`的形状改为(3, 4)，也就是一个3行4列的矩阵，并记作`X`。除了形状改变之外，`X`中的元素保持不变。


```python
X = tf.reshape(x,(3,4))
print(X)
```


    tensor([[ 0,  1,  2,  3],
            [ 4,  5,  6,  7],
            [ 8,  9, 10, 11]])

注意X属性中的形状发生了变化。上面`x.reshape((3, 4))`也可写成`x.reshape((-1, 4))`或`x.reshape((3, -1))`。由于`x`的元素个数是已知的，这里的-1是能够通过元素个数和其他维度的大小推断出来的。

接下来，我们创建一个各元素为0，形状为(2, 3, 4)的张量。实际上，之前创建的向量和矩阵都是特殊的张量。


```python
tf.zeros((2,3,4))
```

```
tensor([[[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]]])
```


类似地，我们可以创建各元素为1的张量。


```python
tf.ones((3,4))
```

```
tensor([[1., 1., 1., 1.],
        [1., 1., 1., 1.],
        [1., 1., 1., 1.]])
```



我们也可以通过Python的列表（list）指定需要创建的`tensor`中每个元素的值。


```python
Y = torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
print(Y)
```

```
tensor([[2, 1, 4, 3],
        [1, 2, 3, 4],
        [4, 3, 2, 1]])
```



有些情况下，我们需要随机生成`tensor`中每个元素的值。下面我们创建一个形状为(3, 4)的`tensor`。它的每个元素都随机采样于均值为0、标准差为1的正态分布。


```python
print(torch.randn(3, 4))
```

```
tensor([[ 1.0070, -0.2015,  0.3744,  0.7960],
        [-0.7998,  0.6268,  0.4862, -0.2268],
        [ 0.5583,  0.5322, -0.7755,  1.2299]])
```



## 运算

`tensor`支持大量的运算符（operator）。例如，我们可以对之前创建的两个形状为(3, 4)的`tensor`做按元素加法。所得结果形状不变。


```python
X + Y
```

```
tensor([[ 2,  2,  6,  6],
        [ 5,  7,  9, 11],
        [12, 12, 12, 12]])
```



按元素乘法：


```python
X * Y
```

```
tensor([[ 0,  1,  8,  9],
        [ 4, 10, 18, 28],
        [32, 27, 20, 11]])
```



按元素除法：


```python
X / Y
```

```
tensor([[ 0.0000,  1.0000,  0.5000,  1.0000],
        [ 4.0000,  2.5000,  2.0000,  1.7500],
        [ 2.0000,  3.0000,  5.0000, 11.0000]])
```



按元素做指数运算：


```python
Y = Y.type(torch.float32)
print(torch.exp(Y))
```

```
tensor([[ 7.3891,  2.7183, 54.5981, 20.0855],
        [ 2.7183,  7.3891, 20.0855, 54.5981],
        [54.5981, 20.0855,  7.3891,  2.7183]])
```



除了按元素计算外，我们还可以使用`matmul`函数做矩阵乘法。下面将`X`与`Y`的转置做矩阵乘法。由于`X`是3行4列的矩阵，`Y`转置为4行3列的矩阵，因此两个矩阵相乘得到3行3列的矩阵。


```python
Y = Y.type(torch.long)
torch.matmul(X, Y.T)
```

```
tensor([[ 18,  20,  10],
        [ 58,  60,  50],
        [ 98, 100,  90]])
```



我们也可以将多个`tensor`连结（concatenate）。下面分别在行上（维度0，即形状中的最左边元素）和列上（维度1，即形状中左起第二个元素）连结两个矩阵。可以看到，输出的第一个`tensor`在维度0的长度（ 6 ）为两个输入矩阵在维度0的长度之和（ 3+3 ），而输出的第二个`tensor`在维度1的长度（ 8 ）为两个输入矩阵在维度1的长度之和（ 4+4 ）。


```python
torch.cat([X, Y], 0), torch.cat((X, Y), 1)
```

```
tensor([[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11],
        [ 2,  1,  4,  3],
        [ 1,  2,  3,  4],
        [ 4,  3,  2,  1]]) tensor([[ 0,  1,  2,  3,  2,  1,  4,  3],
        [ 4,  5,  6,  7,  1,  2,  3,  4],
        [ 8,  9, 10, 11,  4,  3,  2,  1]])
```



使用条件判断式可以得到元素为0或1的新的`tensor`。以X == Y为例，如果X和Y在相同位置的条件判断为真（值相等），那么新的`tensor`在相同位置的值为1；反之为0。


```python
torch.equal(X, Y)
```

```
False
```

对`tensor`中的所有元素求和得到只有一个元素的`tensor`。


```python
torch.sum(X)
```

```
tensor(66)
```


```python
X = X.type(torch.float32)
print(torch.mean(X))
```

```
tensor(5.5000)
```



## 广播机制

前面我们看到如何对两个形状相同的`tensor`做按元素运算。当对两个形状不同的`tensor`按元素运算时，可能会触发广播（broadcasting）机制：先适当复制元素使这两个`tensor`形状相同后再按元素运算。

定义两个`tensor`：


```python
A = torch.reshape(torch.tensor(range(3)), (3, 1))
B = torch.reshape(torch.tensor(range(2)), (1, 2))
print(A, B)
```

```
tensor([[0],
        [1],
        [2]]) tensor([[0, 1]])
```



由于`A`和`B`分别是3行1列和1行2列的矩阵，如果要计算`A + B`，那么A中第一列的3个元素被广播（复制）到了第二列，而B中第一行的2个元素被广播（复制）到了第二行和第三行。如此，就可以对2个3行2列的矩阵按元素相加。


```python
A + B
```

```
tensor([[0, 1],
        [1, 2],
        [2, 3]])
```



## 索引

在`tensor`中，索引（index）代表了元素的位置。`tensor`的索引从0开始逐一递增。例如，一个3行2列的矩阵的行索引分别为0、1和2，列索引分别为0和1。

在下面的例子中，我们指定了`tensor`的行索引截取范围[1:3]。依据左闭右开指定范围的惯例，它截取了矩阵`X`中行索引为1和2的两行。


```python
X[1:3]
```

```
tensor([[ 4.,  5.,  6.,  7.],
        [ 8.,  9., 10., 11.]])
```

我们可以指定`tensor`中需要访问的单个元素的位置，如矩阵中行和列的索引，并为该元素重新赋值。


```python
print(X[1, 2])
X[1, 2] = 9
print(X[1, 2])
```

```
tensor(6.)
tensor(9.)
```

当然，我们也可以截取一部分元素，并为它们重新赋值。在下面的例子中，我们为行索引为1的每一列元素重新赋值。


```python
X
```

```
tensor([[ 0.,  1.,  2.,  3.],
        [ 4.,  5.,  9.,  7.],
        [ 8.,  9., 10., 11.]])
```




```python
X[1:2, :] = 12
print(X)
```

```
tensor([[ 0.,  1.,  2.,  3.],
        [12., 12., 12., 12.],
        [ 8.,  9., 10., 11.]])
```



## 运算的内存开销

在前面的例子里我们对每个操作新开内存来存储运算结果。举个例子，即使像`Y = X + Y`这样的运算，我们也会新开内存，然后将`Y`指向新内存。为了演示这一点，我们可以使用Python自带的`id`函数：如果两个实例的ID一致，那么它们所对应的内存地址相同；反之则不同。


```python
Y = Y.type(torch.float32)

before = id(Y)
Y = Y + X
print(id(Y) == before)
```

```
False
```


如果想指定结果到特定内存，我们可以使用前面介绍的索引来进行替换操作。在下面的例子中，我们先通过`zeros_like`创建和`Y`形状相同且元素为0的`tensor`，记为`Z`。接下来，我们把`X + Y`的结果通过`[:]`写进`Z`对应的内存中。


```python
Z = torch.zeros_like(Y)
before = id(Z)
Z[:] = X + Y
print(id(Z) == before)
```




    True

实际上，上例中我们还是为`X + Y`开了临时内存来存储计算结果，再复制到`Z`对应的内存。如果想避免这个临时内存开销，我们可以使用`=`直接赋值。


```python
Z = torch.add(X, Y)
print(id(Z) == before)
```




    False



如果`X`的值在之后的程序中不会复用，我们也可以用 `X[:] = X + Y` 或者 `X += Y` 来减少运算的内存开销。


```python
before = id(X)
X += Y
print(id(X) == before)
```




    True



## tensor 和 NumPy 相互变换

我们可以通过array函数和asnumpy函数令数据在NDArray和NumPy格式之间相互变换。下面将NumPy实例变换成tensor实例。


```python
import numpy as np

P = np.ones((2, 3))
D = torch.tensor(P)
print(D)
```




    tensor([[1., 1., 1.],
            [1., 1., 1.]], dtype=torch.float64)



再将NDArray实例变换成NumPy实例。


```python
np.array(D)
```




    array([[1., 1., 1.],
           [1., 1., 1.]])

