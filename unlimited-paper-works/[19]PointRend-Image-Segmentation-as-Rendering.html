<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>PointRend: Image Segmentation as Rendering | 工具箱的深度学习记事簿</title>
    <meta name="generator" content="VuePress 1.8.2">
    <link rel="icon" href="/statics/logo.svg">
    <meta name="description" content="这里包含了我从入门到依然在入门的过程中接触到的大部分知识。翻翻目录，也许能找到有用的">
    <meta name="keywords" content="Akasaki,Deep learning,Machine learning,工具箱,工具箱的深度学习记事簿,Akasaki的深度学习记事簿">
    <meta name="google-site-verification" content="VVNYs0bXM_EKTgxJ8XIfvXShjHsksGNv3YNedxBGFjU">
    
    <link rel="preload" href="/assets/css/0.styles.b2187a0e.css" as="style"><link rel="preload" href="/assets/js/app.0cf0d483.js" as="script"><link rel="preload" href="/assets/js/2.d537feee.js" as="script"><link rel="preload" href="/assets/js/18.582ac004.js" as="script"><link rel="prefetch" href="/assets/js/10.000b9996.js"><link rel="prefetch" href="/assets/js/100.01b72e43.js"><link rel="prefetch" href="/assets/js/101.ab99304f.js"><link rel="prefetch" href="/assets/js/102.b1ad1ca3.js"><link rel="prefetch" href="/assets/js/11.0ec32f4a.js"><link rel="prefetch" href="/assets/js/12.7ad41a4b.js"><link rel="prefetch" href="/assets/js/13.40c87056.js"><link rel="prefetch" href="/assets/js/14.2d8bcb8d.js"><link rel="prefetch" href="/assets/js/15.a7295b50.js"><link rel="prefetch" href="/assets/js/16.7158147a.js"><link rel="prefetch" href="/assets/js/17.9215c05c.js"><link rel="prefetch" href="/assets/js/19.7cc8fc31.js"><link rel="prefetch" href="/assets/js/20.1b1e2af4.js"><link rel="prefetch" href="/assets/js/21.b2dadf61.js"><link rel="prefetch" href="/assets/js/22.b19748fc.js"><link rel="prefetch" href="/assets/js/23.f2db4fa1.js"><link rel="prefetch" href="/assets/js/24.3c31aece.js"><link rel="prefetch" href="/assets/js/25.531fd9fb.js"><link rel="prefetch" href="/assets/js/26.6e841782.js"><link rel="prefetch" href="/assets/js/27.33add335.js"><link rel="prefetch" href="/assets/js/28.35498a04.js"><link rel="prefetch" href="/assets/js/29.aabed81c.js"><link rel="prefetch" href="/assets/js/3.0a121536.js"><link rel="prefetch" href="/assets/js/30.54f44e16.js"><link rel="prefetch" href="/assets/js/31.8ecbe721.js"><link rel="prefetch" href="/assets/js/32.873f263a.js"><link rel="prefetch" href="/assets/js/33.106c15c8.js"><link rel="prefetch" href="/assets/js/34.349d6fdb.js"><link rel="prefetch" href="/assets/js/35.73569c69.js"><link rel="prefetch" href="/assets/js/36.24f08107.js"><link rel="prefetch" href="/assets/js/37.da460b69.js"><link rel="prefetch" href="/assets/js/38.2f6c4646.js"><link rel="prefetch" href="/assets/js/39.435f8032.js"><link rel="prefetch" href="/assets/js/4.972cd905.js"><link rel="prefetch" href="/assets/js/40.4760c47b.js"><link rel="prefetch" href="/assets/js/41.a7cc5e5d.js"><link rel="prefetch" href="/assets/js/42.fc3e47f2.js"><link rel="prefetch" href="/assets/js/43.a13c67d4.js"><link rel="prefetch" href="/assets/js/44.46c317cb.js"><link rel="prefetch" href="/assets/js/45.f11cdfc6.js"><link rel="prefetch" href="/assets/js/46.541bd27d.js"><link rel="prefetch" href="/assets/js/47.5705cba6.js"><link rel="prefetch" href="/assets/js/48.d2040c65.js"><link rel="prefetch" href="/assets/js/49.cbb5c42b.js"><link rel="prefetch" href="/assets/js/5.e2cb1c20.js"><link rel="prefetch" href="/assets/js/50.39732f79.js"><link rel="prefetch" href="/assets/js/51.99b82adb.js"><link rel="prefetch" href="/assets/js/52.808c9ece.js"><link rel="prefetch" href="/assets/js/53.67e417da.js"><link rel="prefetch" href="/assets/js/54.1c722ed6.js"><link rel="prefetch" href="/assets/js/55.afc1aceb.js"><link rel="prefetch" href="/assets/js/56.a463c15a.js"><link rel="prefetch" href="/assets/js/57.f90cb51c.js"><link rel="prefetch" href="/assets/js/58.8e11cf82.js"><link rel="prefetch" href="/assets/js/59.ed4fc789.js"><link rel="prefetch" href="/assets/js/6.59e5684c.js"><link rel="prefetch" href="/assets/js/60.adcefa1d.js"><link rel="prefetch" href="/assets/js/61.a6446644.js"><link rel="prefetch" href="/assets/js/62.1721a24b.js"><link rel="prefetch" href="/assets/js/63.726a6135.js"><link rel="prefetch" href="/assets/js/64.e2db755f.js"><link rel="prefetch" href="/assets/js/65.e92b9d46.js"><link rel="prefetch" href="/assets/js/66.b4d3e87e.js"><link rel="prefetch" href="/assets/js/67.5f3eaf29.js"><link rel="prefetch" href="/assets/js/68.85b4615c.js"><link rel="prefetch" href="/assets/js/69.63d409f1.js"><link rel="prefetch" href="/assets/js/7.b3ddb4c6.js"><link rel="prefetch" href="/assets/js/70.6f3f7332.js"><link rel="prefetch" href="/assets/js/71.0a087c8a.js"><link rel="prefetch" href="/assets/js/72.6513570f.js"><link rel="prefetch" href="/assets/js/73.896f4d9c.js"><link rel="prefetch" href="/assets/js/74.ecc3d286.js"><link rel="prefetch" href="/assets/js/75.b06a8964.js"><link rel="prefetch" href="/assets/js/76.3032add1.js"><link rel="prefetch" href="/assets/js/77.4fb4de51.js"><link rel="prefetch" href="/assets/js/78.42cf5412.js"><link rel="prefetch" href="/assets/js/79.0ba88830.js"><link rel="prefetch" href="/assets/js/8.bb97c23d.js"><link rel="prefetch" href="/assets/js/80.aaa6656c.js"><link rel="prefetch" href="/assets/js/81.4a1804f1.js"><link rel="prefetch" href="/assets/js/82.7ad53f0e.js"><link rel="prefetch" href="/assets/js/83.c4173111.js"><link rel="prefetch" href="/assets/js/84.cb7300ea.js"><link rel="prefetch" href="/assets/js/85.81c0ea84.js"><link rel="prefetch" href="/assets/js/86.317fcc1c.js"><link rel="prefetch" href="/assets/js/87.6f50a42f.js"><link rel="prefetch" href="/assets/js/88.898a4cd8.js"><link rel="prefetch" href="/assets/js/89.ddbe5c5c.js"><link rel="prefetch" href="/assets/js/9.596e7f45.js"><link rel="prefetch" href="/assets/js/90.c1e40160.js"><link rel="prefetch" href="/assets/js/91.f85fdec8.js"><link rel="prefetch" href="/assets/js/92.1d7822a2.js"><link rel="prefetch" href="/assets/js/93.4ee2ef21.js"><link rel="prefetch" href="/assets/js/94.3760a5b4.js"><link rel="prefetch" href="/assets/js/95.b5be500a.js"><link rel="prefetch" href="/assets/js/96.e570f48e.js"><link rel="prefetch" href="/assets/js/97.af175cfb.js"><link rel="prefetch" href="/assets/js/98.ca3ecf81.js"><link rel="prefetch" href="/assets/js/99.2fc27711.js">
    <link rel="stylesheet" href="/assets/css/0.styles.b2187a0e.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><!----> <span class="site-name">工具箱的深度学习记事簿</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="https://github.com/VisualDust/ml.akasaki.space" target="_blank" rel="noopener noreferrer" class="nav-link external">
  View on Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div><div class="nav-item"><a href="https://github.com/VisualDust" target="_blank" rel="noopener noreferrer" class="nav-link external">
  工具箱
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="https://github.com/VisualDust/ml.akasaki.space" target="_blank" rel="noopener noreferrer" class="nav-link external">
  View on Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div><div class="nav-item"><a href="https://github.com/VisualDust" target="_blank" rel="noopener noreferrer" class="nav-link external">
  工具箱
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第零章：在开始之前</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第一章上：HelloWorld</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第一章下：深度学习基础</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第二章上：卷积神经网络</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第二章下：经典卷积神经网络</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第三章上：谈一些计算机视觉方向</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第三章下：了解更高级的技术</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>魔法部日志（又名论文阅读日志）</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/unlimited-paper-works/[0]unlimited-paper-works.html" class="sidebar-link">欢迎来到魔法部日志</a></li><li><a href="/unlimited-paper-works/[1]The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs.html" class="sidebar-link">The Devil is in the Decoder: Classification, Regression and GANs</a></li><li><a href="/unlimited-paper-works/[2]Threat-of-Adversarial-Attacks-on-Deep-Learning-in-Computer-Vision-A-Survey.html" class="sidebar-link">Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey</a></li><li><a href="/unlimited-paper-works/[3]Progressive-Semantic-Segmentation.html" class="sidebar-link">Progressive Semantic Segmentation</a></li><li><a href="/unlimited-paper-works/[4]Decoders-Matter-for-Semantic-Segmentation-Data-Dependent-Decoding-Enables-Flexible-Feature-Aggregation.html" class="sidebar-link">Decoders Matter for Semantic Segmentation: Data-Dependent Decoding Enables Flexible Feature Aggregation</a></li><li><a href="/unlimited-paper-works/[5]HLA-Face-Joint-High-Low-Adaptation-for-Low-Light-Face-Detection.html" class="sidebar-link">HLA-Face Joint High-Low Adaptation for Low Light Face Detection</a></li><li><a href="/unlimited-paper-works/[6]DeepLab-Series.html" class="sidebar-link">DeepLab Series</a></li><li><a href="/unlimited-paper-works/[7]Cross-Dataset-Collaborative-Learning-for-Semantic-Segmentation.html" class="sidebar-link">Cross-Dataset Collaborative Learning for Semantic Segmentation</a></li><li><a href="/unlimited-paper-works/[8]Dynamic-Neural-Networks-A-Survey.html" class="sidebar-link">Dynamic Neural Networks: A Survey</a></li><li><a href="/unlimited-paper-works/[9]Feature-Pyramid-Networks-for-Object-Detection.html" class="sidebar-link">Feature Pyramid Networks for Object Detection</a></li><li><a href="/unlimited-paper-works/[10]Overview-Of-Semantic-Segmentation.html" class="sidebar-link">A Review on Deep Learning Techniques Applied to Semantic Segmentation</a></li><li><a href="/unlimited-paper-works/[11]Image-Segmentation-Using-Deep-Learning-A-Survey.html" class="sidebar-link">Image Segmentation Using Deep Learning: A Survey</a></li><li><a href="/unlimited-paper-works/[12]MobileNetV2-Inverted-Residuals-and-Linear-bottleneck.html" class="sidebar-link">MobileNetV2: Inverted Residuals and Linear Bottlenecks</a></li><li><a href="/unlimited-paper-works/[13]Fast-SCNN-Fast-Semantic-Segmentation-Network.html" class="sidebar-link">Fast-SCNN: Fast Semantic Segmentation Network</a></li><li><a href="/unlimited-paper-works/[14]MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications.html" class="sidebar-link">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</a></li><li><a href="/unlimited-paper-works/[15]Gated-Channel-Transformation-for-Visual-Recognition.html" class="sidebar-link">Gated Channel Transformation for Visual Recognition</a></li><li><a href="/unlimited-paper-works/[16]Convolutional-Block-Attention-Module.html" class="sidebar-link">Convolutional Block Attention Module</a></li><li><a href="/unlimited-paper-works/[17]Boundary-IoU-Improving-Object-Centri Image-Segmentation-Evaluation.html" class="sidebar-link">Boundary IoU: Improving Object-Centric Image Segmentation Evaluation</a></li><li><a href="/unlimited-paper-works/[18]Involution-Inverting-the-Inherence-of-Convolution-for-Visual-Recognition.html" class="sidebar-link">Involution: Inverting the Inherence of Convolution for Visual Recognition</a></li><li><a href="/unlimited-paper-works/[19]PointRend-Image-Segmentation-as-Rendering.html" class="active sidebar-link">PointRend: Image Segmentation as Rendering</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[19]PointRend-Image-Segmentation-as-Rendering.html#abstract-摘要" class="sidebar-link">Abstract（摘要）</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[19]PointRend-Image-Segmentation-as-Rendering.html#介绍-introduction" class="sidebar-link">介绍（Introduction）</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[19]PointRend-Image-Segmentation-as-Rendering.html#方法-method" class="sidebar-link">方法（Method）</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[19]PointRend-Image-Segmentation-as-Rendering.html#pointrend模块" class="sidebar-link">PointRend模块</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[19]PointRend-Image-Segmentation-as-Rendering.html#pointrend模块的组成" class="sidebar-link">PointRend模块的组成</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[19]PointRend-Image-Segmentation-as-Rendering.html#点的选择策略-point-selection-for-inference-and-training" class="sidebar-link">点的选择策略（Point selection for Inference and Training）</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[19]PointRend-Image-Segmentation-as-Rendering.html#点的特征提取和分类-point-wise-representation-and-point-head" class="sidebar-link">点的特征提取和分类（Point-wise Representation and Point Head）</a></li></ul></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[19]PointRend-Image-Segmentation-as-Rendering.html#实验-experiments" class="sidebar-link">实验（Experiments）</a></li></ul></li><li><a href="/unlimited-paper-works/[20]Transformer-Attention-is-all-you-need.html" class="sidebar-link">Transformer: Attention is all you need</a></li><li><a href="/unlimited-paper-works/[21]RefineMask_Towards_High-Quality_Instance_Segmentationwith_Fine-Grained_Features.html" class="sidebar-link">RefineMask: Towards High-Quality Instance Segmentationwith Fine-Grained Features</a></li><li><a href="/unlimited-paper-works/[22]GLADNet-Low-Light-Enhancement-Network-with-Global-Awareness.html" class="sidebar-link">GLADNet: Low-Light Enhancement Network with Global Awareness</a></li><li><a href="/unlimited-paper-works/[23]Squeeze-and-Excitation-Networks.html" class="sidebar-link">Squeeze-and-Excitation Networks (SENet)</a></li><li><a href="/unlimited-paper-works/[24]BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation.html" class="sidebar-link">BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation</a></li><li><a href="/unlimited-paper-works/[25]Rethinking-BiSeNet-For-Real-time-Semantic-Segmentation.html" class="sidebar-link">Rethinking BiSeNet For Real-time Semantic Segmentation</a></li><li><a href="/unlimited-paper-works/[26]CBAM-Convolutional-Block-Attention-Module.html" class="sidebar-link">CBAM: Convolutional Block Attention Module</a></li><li><a href="/unlimited-paper-works/[27]Non-local-Neural-Networks.html" class="sidebar-link">Non-local Neural Networks</a></li><li><a href="/unlimited-paper-works/[28]GCNet-Non-local-Networks-Meet-Squeeze-Excitation-Networks-and-Beyond.html" class="sidebar-link">GCNet: Global Context Networks (Non-local Networks Meet Squeeze-Excitation Networks and Beyond)</a></li><li><a href="/unlimited-paper-works/[29]Disentangled-Non-Local-Neural-Networks.html" class="sidebar-link">Disentangled Non-Local Neural Networks</a></li><li><a href="/unlimited-paper-works/[30]RetinexNet-for-Low-Light-Enhancement.html" class="sidebar-link">Deep Retinex Decomposition for Low-Light Enhancement</a></li><li><a href="/unlimited-paper-works/[31]MSR-netLow-light-Image-Enhancement-Using-Deep-Convolutional-Network.html" class="sidebar-link">MSR-net:Low-light Image Enhancement Using Deep Convolutional Network</a></li><li><a href="/unlimited-paper-works/[32]LLCNN-A-Convolutional-Neural-Network-for-Low-light-Image-Enhancement.html" class="sidebar-link">LLCNN: A convolutional neural network for low-light image enhancement</a></li></ul></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>附录1：好朋友们</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>附录2：数学是真正的圣经</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>附录3：信号和采样的学问（DSP）</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>附录4：TensorFlow编程策略</span> <span class="arrow right"></span></p> <!----></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="pointrend-image-segmentation-as-rendering"><a href="#pointrend-image-segmentation-as-rendering" class="header-anchor">#</a> PointRend: Image Segmentation as Rendering</h1> <p><img src="/assets/img/image-20210601121147760.d589876a.png" alt="image-20210601121147760"></p> <h3 id="这篇笔记的写作者是visualdust。"><a href="#这篇笔记的写作者是visualdust。" class="header-anchor">#</a> 这篇笔记的写作者是<a href="https://github.com/visualDust" target="_blank" rel="noopener noreferrer">VisualDust<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>。</h3> <blockquote><p>“我们希望预测分割图的边界区域更加准确，我们就不应该使用均匀采样，而应该更加倾向于图像边界区域。”</p></blockquote> <p>这是一篇用于改善图像分割问题中边缘分割效果的方法的论文的阅读笔记。该方法“将分割问题看作渲染问题”，达到了较好的效果。论文原文：<a href="https://arxiv.org/abs/1912.08193" target="_blank" rel="noopener noreferrer">PointRend: Image Segmentation as Rendering<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>。在阅读这篇笔记之前，请确保先了解图像分割技术。对分割的技术进行简要的了解，可以参考<a href="/unlimited-paper-works/[10]Overview-Of-Semantic-Segmentation.html">另一篇笔记</a>。</p> <h2 id="abstract-摘要"><a href="#abstract-摘要" class="header-anchor">#</a> Abstract（摘要）</h2> <blockquote><p>We present a new method for efficient high-quality image segmentation of objects and scenes. By analogizing classical computer graphics methods for efficient rendering with over- and undersampling challenges faced in pixel labeling tasks, we develop a unique perspective of image segmentation as a rendering problem. From this vantage, we present the PointRend (Point-based Rendering) neural network module: a module that performs point-based segmentation predictions at adaptively selected locations based on an iterative subdivision algorithm. PointRend can be flexibly applied to both instance and semantic segmentation tasks by building on top of existing state-of-the-art models. While many concrete implementations of the general idea are possible, we show that a simple design already achieves excellent results. Qualitatively, PointRend outputs crisp object boundaries in regions that are over-smoothed by previous methods. Quantitatively, PointRend yields significant gains on COCO and Cityscapes, for both instance and semantic segmentation. PointRend's efficiency enables output resolutions that are otherwise impractical in terms of memory or computation compared to existing approaches. Code has been made available at <a href="https://github.com/facebookresearch/detectron2/tree/master/projects/PointRend" target="_blank" rel="noopener noreferrer">this https URL<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>.</p></blockquote> <h2 id="介绍-introduction"><a href="#介绍-introduction" class="header-anchor">#</a> 介绍（Introduction）</h2> <p>我们希望预测分割图的边界区域更加准确，我们就不应该使用均匀采样，而应该更加倾向于图像边界区域。这种类似的采样问题在计算机图形学中已经被研究了几十年了，图像渲染，将一个模型（比如，3D网格）映射为一个rasterized image（即一个像素的规则网格），虽然输出是规则的，但是计算的时候却不是根据网格上均匀采样来计算。常见的策略就是在图像平面内自适应地采样一些点产生不规则的子集，再来进行计算。</p> <p>这篇文章的中心思想就是将图像分割问题视作图像渲染问题，使用来自于计算机图像学中的经典思想设计到神经网络中，渲染出更高质量的分割图。这个设计的核心内容是 PointRend（基于点的渲染）神经网络模块：“一个基于迭代细分算法在自适应选择的位置执行基于点的分割预测的模块”。该模块使用细分策略自适应地选择一组非均匀点来计算标签，容易理解的说法是该模块自动选取边缘上难以正确归类的点进行再次分类。 它的细分策略使用比直接密集计算少一个数量级的浮点运算来有效地计算高分辨率分割图。</p> <p>请注意，PointRend是一个网络模块而不是独立的网络。该模块接受一个或多个CNN输出的feature map，并产生比输入更高分辨率的预测。</p> <blockquote><p>PointRend is a general module that admits many possible implementations.</p></blockquote> <p>PointRend 可以合并到流行的元架构中，用于实例分割（例如，Mask R-CNN）和语义分割（例如FCN）。</p> <p><img src="/assets/img/image-20210601154033700.4e61ea78.png" alt="image-20210601154033700"></p> <p>上图是一种可能的PointRend结构模式图：选择一些分割困难的点，CNN产生的特征图被输入到MLP中，和粗分割结果融合，最终输出分割困难点的预测结果。</p> <p>实际上，这篇论文的创新在于重新选择边缘困难点的种类。本质上这篇论文其实是一个新型上采样方法，针对物体边缘的图像分割进行优化，使其在难以分割的物体边缘部分有更好的表现。在整个PointRend设计的过程中有类似于渲染的思想，但请不要对“渲染”过度理解。</p> <hr> <h2 id="方法-method"><a href="#方法-method" class="header-anchor">#</a> 方法（Method）</h2> <h3 id="pointrend模块"><a href="#pointrend模块" class="header-anchor">#</a> PointRend模块</h3> <p>类似于在计算机图形学中，屏幕上某个位置的像素是从一个曲线、模型等通过被称为渲染的映射产生，在计算机视觉中，我们可以将图像分割视为底层连续实体的类别图，而分割输出，即预测标签的像素集合，是从中“渲染”出来的。</p> <p><img src="/assets/img/image-20210601161409751.4d8aedc7.png" alt="image-20210601161409751"></p> <p>上图是这个模式的简图。其中，PointRend的关键步骤就是训练一个“解码器”（或者叫“渲染器”），从channel中“解码”（或者“渲染”）出预测困难的像素所属的类别。</p> <p>PointRend模块接受一个或多个典型的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span>通道的CNN特征图（<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>C</mi><mo>×</mo><mi>H</mi><mo>×</mo><mi>W</mi></mrow></msup></mrow><annotation encoding="application/x-tex">f\in \R^{C\times H\times W}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8413309999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">C</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.08125em;">H</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">W</span></span></span></span></span></span></span></span></span></span></span></span>）作为输入，这些输入特征图往往比需要预测的图像的实际尺寸要小4~16倍。PointRend模块会输出一个大小为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>k</mi><mo>×</mo><msup><mi>H</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>×</mo><msup><mi>W</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow></msup></mrow><annotation encoding="application/x-tex">p\in \R^{k\times H' \times W'}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.7335400000000001em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">p</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.94248em;vertical-align:0em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.94248em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mbin mtight">×</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8278285714285715em;"><span style="top:-2.931em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mbin mtight">×</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8278285714285715em;"><span style="top:-2.931em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>的对K个类别的预测。通常输出的大小会大于输入的大小。</p> <p>（如果读不懂了请先参考<a href="/unlimited-paper-works/[10]Overview-Of-Semantic-Segmentation.html">另一篇笔记</a>了解图像的分割技术）</p> <h3 id="pointrend模块的组成"><a href="#pointrend模块的组成" class="header-anchor">#</a> PointRend模块的组成</h3> <p>PointRend模块由单个主要的部分组成：</p> <ol><li>一个<strong>点的选择策略</strong>。PointRend模块并不会对整幅图片上的所有点进行预测（这样会产生巨大的开销），而是选择其中的一部分看上去“难以预测的实值点”进行预测（实值点的特征是通过输入<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span></span>的双线性插值计算的）。</li> <li>一个<strong>特征提取器</strong>（或者叫“解码器”，或是“渲染器”）。对于每个选定的点，在输入中相关的部分可能是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>C</mi><mo>×</mo><mn>1</mn><mo>×</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">i\in \R^{C\times 1\times 1}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.69862em;vertical-align:-0.0391em;"></span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8413309999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">C</span><span class="mbin mtight">×</span><span class="mord mtight">1</span><span class="mbin mtight">×</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span>的一个长长的通道。对于每个选定的点，特征提取器提取该点的特征表示。</li> <li>一个<strong>分类器</strong>（原文中称之为“point head”）。这个分类器是一个很小的神经网络，它被训练来从这个逐点特征表示中预测一个标签。对于这部分来说，每个点都是独立的。</li></ol> <p>这几个组成部分将会在下文中进行详细介绍。</p> <p>PointRend 架构可应用于实例分割（例如，在 Mask R-CNN上）和语义分割（例如，在 FCNs上）任务。</p> <p><img src="/assets/img/image-20210601154033700.4e61ea78.png" alt="image-20210601154033700"></p> <p>如上图，PointRend 模块应用于每个区域。它通过对一组选定点进行预测，以从粗到细的方式计算掩码。对于语义分割，可以将整个图像视为单个区域，因此不失一般性。</p> <h3 id="点的选择策略-point-selection-for-inference-and-training"><a href="#点的选择策略-point-selection-for-inference-and-training" class="header-anchor">#</a> 点的选择策略（Point selection for Inference and Training）</h3> <p>PointRend方法的核心是灵活和自适应地选择图像平面中预测分割标签的点。直觉上，这些点应该更密集地靠近高频区域，例如物体边界，类似于光线追踪中的抗锯齿问题。</p> <p>PointRend的推理选择策略受到计算机图形学中自适应细分的经典技术的启发。该技术用于通过仅在值与其邻居显著不同的位置进行计算来高效地渲染高分辨率图像。对于其他位置，这些值是通过插入已经计算出的输出值获得的。下面是一段解释：</p> <p><img src="/assets/img/image-20210601170513086.ecf6a887.png" alt="image-20210601170513086"></p> <p>在渲染问题中，例如，在一块分辨率为1080p的屏幕上显示一张2k的图片时，往往显示设备不需要对2k图片的所有像素都进行处理，仅处理一部分即可显示出足够的分辨率。但当用户放大图片，在1080p的显示屏上显示一张2k图片中的一角时，显示设备就需要对这张图片进行更精细的处理让它们出现在屏幕上显示更多细节。通过观察我们发现，1和2部分和原来的像素差距不大，不需要从图片文件渲染，处理时仅需同上文中的“这些值是通过插入已经计算出的输出值获得的”，从放大前的图上插值即可得到。需要重新处理的，是1和2的边缘部分，也就是上面提到的“值与其邻居显著不同的位置”。</p> <blockquote><p>类比渲染问题中的细分过程和分割问题中的上采样过程后我们发现，在分割中的上采样过程中时我们需要关注边缘，对边缘进行细化即可。</p></blockquote> <p>所以点的选择策略，就是在上采样过程中选择“不确定的点”，或“边界的点”。</p> <h4 id="前向传播过程-inference"><a href="#前向传播过程-inference" class="header-anchor">#</a> 前向传播过程（Inference）</h4> <blockquote><p>对于每个区域，我们以粗到细的方式迭代地“渲染”输出掩码。</p></blockquote> <p>在网络的前向传播过程到达PointRend模块时，会总体经历以下步骤：</p> <ol><li>使用双线性插值对其先前预测的分割进行上采样。</li> <li>在这个上采样的特征图上选择 N 个最不确定的点（例如，对于二进制掩码，可以选择概率最接近 0.5 的那些点）。</li> <li>为这 N 个点中的每一个计算逐点特征表示（在第 3.2 节中简要描述）并预测它们的标签。</li> <li>重复这个过程，直到分割被上采样到所需的分辨率。</li></ol> <p><img src="/assets/img/image-20210601200340192.3073ab01.png" alt="image-20210601200340192"></p> <p>上图是这个过程的放在整个网络中的大致流程示意图，下面是这个过程的局部流程图。</p> <p><img src="/assets/img/image-20210601205123801.f08c6d9f.png" alt="image-20210601205123801"></p> <p>整个过程就是比双线性插值的上采样多了一个选择不确定点和预测其种类的过程。整个前向传播的过程简单易懂。</p> <p>输入大小为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>M</mi><mn>0</mn></msub><mo>×</mo><msub><mi>M</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">M_0 \times M_0</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>并且输出大小为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>×</mo><mi>M</mi></mrow><annotation encoding="application/x-tex">M\times M</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span></span></span>的PointRend模块一次运算所需要预测的像素总量不会超过<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><msub><mrow><mi>log</mi><mo>⁡</mo></mrow><mn>2</mn></msub><mfrac><mi>M</mi><msub><mi>M</mi><mn>0</mn></msub></mfrac></mrow><annotation encoding="application/x-tex">N \log_2 {\frac{M}{M_0}}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:1.317431em;vertical-align:-0.44509999999999994em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop"><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.20696799999999996em;"><span style="top:-2.4558600000000004em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.24414em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.872331em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:-0.10903em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">M</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.44509999999999994em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>，这比<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>×</mo><mi>M</mi></mrow><annotation encoding="application/x-tex">M\times M</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span></span></span>小得多，并使PointRend 更有效地进行高分辨率预测。例如，如果<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>M</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">M_0</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>为 7 并且所需分辨率为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>=</mo><mn>224</mn></mrow><annotation encoding="application/x-tex">M =224</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">224</span></span></span></span> ，则执行 5 个细分步骤。如果我们在每一步选择<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mn>2</mn><msup><mn>8</mn><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">N =28^2</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord"><span class="mord">8</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>个点，PointRend 只对$ 28^2 · 4.25 $个点进行预测，这比<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>22</mn><msup><mn>4</mn><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">224^2</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord">22</span><span class="mord"><span class="mord">4</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>小 15 倍。</p> <h4 id="训练时的-不确定-点选取-training"><a href="#训练时的-不确定-点选取-training" class="header-anchor">#</a> 训练时的“不确定”点选取（Training）</h4> <p>在之前的一些步骤中，我们提到PointRend模块中需要选择一些“不确定”的点进行分类。那么，怎么选择“不确定”的点呢？我们在上面提到:</p> <blockquote><p>在这个上采样的特征图上选择 N 个最不确定的点</p></blockquote> <p><img src="/assets/img/image-20210601154033700-1622553357332.4e61ea78.png" alt="image-20210601154033700"></p> <p>采样策略在特征图上选择N个点，这N个点应该是“不确定”的。PointRend在选择点时使用三个原则使选择到的点是“不确定”的：</p> <ol><li><p>过度生成：平均从输入中选择<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mi>N</mi></mrow><annotation encoding="application/x-tex">kN</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span>个点（其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>&gt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">k&gt;1</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.73354em;vertical-align:-0.0391em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>）。</p></li> <li><p>重要性采样：根据粗预测（上方图片中的“coarse prediction”）结果判断每个点的不确定性，从中选择“不确定粗略预测的点”，从刚才选取的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mi>N</mi></mrow><annotation encoding="application/x-tex">kN</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span>的点中选出最不确定的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi><mi>N</mi></mrow><annotation encoding="application/x-tex">\beta N</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">βN</span></span></span></span>个点（其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">\beta \in [0,1]</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span>）。</p></li> <li><p>Coverage （说实话暂时没看懂这一个是干啥的）：剩余的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>β</mi><mo stretchy="false">)</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">(1-\beta)N</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span>个点是从均匀分布中采样。如下图中采用了不同的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span>和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span></span></span></span>来说明这个过程。左侧是均匀分布的示意图，右侧是采用不同的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span>和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span></span></span></span>的示意图。</p> <p><img src="/assets/img/image-20210601211914002.9e342409.png" alt="image-20210601211914002"></p></li></ol> <p><code>原则上，这里的选择点的策略可以与上文中推理时点的细分策略类似。**然而，细分的策略会引入对神经网络的反向传播算法不太友好的运算步骤**，所以在选择点的时候这篇论文中选择了非迭代的随机采样策略。</code></p> <h3 id="点的特征提取和分类-point-wise-representation-and-point-head"><a href="#点的特征提取和分类-point-wise-representation-and-point-head" class="header-anchor">#</a> 点的特征提取和分类（Point-wise Representation and Point Head）</h3> <p>在特征提取部分，PointRend通过融合粗特征和细粒度特征获得更合适的特征，然后通过分类得到该点的类别。</p> <h4 id="细粒度特征-fine-grained-features"><a href="#细粒度特征-fine-grained-features" class="header-anchor">#</a> 细粒度特征（Fine-grained features）</h4> <blockquote><p>细粒度特征就是各种分割网络中用于稠密预测的直接特征。</p></blockquote> <p>为了让 PointRend 呈现精细的分割细节，Point Rend方法中从前序CNN输出的特征图中的每个像素位置提取一个特征向量，然后进行双线性插值上采样作为这个位置像素的细粒度特征。可以从单个特征映射中提取这个特征（例如ResNet中的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mi>e</mi><msub><mi>s</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">res_2</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord mathnormal">re</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>）；或者，也可以从多个特征图融合得到（例如<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mi>e</mi><msub><mi>s</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">res_2</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord mathnormal">re</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>到<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mi>e</mi><msub><mi>s</mi><mn>5</mn></msub></mrow><annotation encoding="application/x-tex">res_5</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord mathnormal">re</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">5</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，或是从它们的特征金字塔对应层的输出提取）。</p> <h4 id="粗预测特征-coarse-prediction-features"><a href="#粗预测特征-coarse-prediction-features" class="header-anchor">#</a> 粗预测特征（Coarse prediction features）</h4> <blockquote><p>粗粒度让特征“更具空间性”。</p></blockquote> <p>单纯使用细粒度特征容易导致先天性的不足，那就是和周围较大范围内的像素很难产生关联，也就是所谓的空间信息丢失。这是前序CNN下采样导致的结果。例如，同时处在两个不同实体边界上的点只具有一份相同的细粒度特征，但是一个点只能被分给一个实体（也换句话说就是具有相同细粒度特征的点在不同的实体区域内应该被分类为不同标签），这就需要该点的额外的区域信息。</p> <p>还有，根据产生细粒度特征所使用的特征图的不同，可能会出现细粒度特征内只包含相对低级的信息的情况。在这种情况下，具有更多上下文和语义信息的特征源会对分割的精度产生很大的帮助。</p> <p>基于这些考虑，第二种特征类型是来自网络的粗分割预测，即区域（框）中每个点的 K 维向量代表 K 类预测。</p> <h4 id="分类器-点头-point-head"><a href="#分类器-点头-point-head" class="header-anchor">#</a> 分类器（“点头”，Point Head）</h4> <p>点头。给定每个选定点的逐点特征表示，PointRend 使用简单的多层感知器 (MLP) 进行逐点分割预测。这个 MLP 在所有点（和所有区域）上共享权重，类似于图卷积或 PointNet。由于 MLP 预测每个点的分割标签，它可以通过标准的特定于任务的分割损失进行训练。</p> <h2 id="实验-experiments"><a href="#实验-experiments" class="header-anchor">#</a> 实验（Experiments）</h2> <p>请参考原文。</p></div> <footer class="page-edit"><!----> <div class="last-updated"><span class="prefix">上次更新时间:</span> <span class="time">2021/7/27 上午4:27:40</span></div></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/unlimited-paper-works/[18]Involution-Inverting-the-Inherence-of-Convolution-for-Visual-Recognition.html" class="prev">
        Involution: Inverting the Inherence of Convolution for Visual Recognition
      </a></span> <span class="next"><a href="/unlimited-paper-works/[20]Transformer-Attention-is-all-you-need.html">
        Transformer: Attention is all you need
      </a>
      →
    </span></p></div> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.0cf0d483.js" defer></script><script src="/assets/js/2.d537feee.js" defer></script><script src="/assets/js/18.582ac004.js" defer></script>
  </body>
</html>
