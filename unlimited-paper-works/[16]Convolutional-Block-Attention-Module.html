<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Convolutional Block Attention Module | 工具箱的深度学习记事簿</title>
    <meta name="generator" content="VuePress 1.8.2">
    <link rel="icon" href="/statics/logo.svg">
    <meta name="description" content="这里包含了我从入门到依然在入门的过程中接触到的大部分知识。翻翻目录，也许能找到有用的">
    <meta name="keywords" content="Akasaki,Deep learning,Machine learning,工具箱,工具箱的深度学习记事簿,Akasaki的深度学习记事簿">
    <meta name="google-site-verification" content="VVNYs0bXM_EKTgxJ8XIfvXShjHsksGNv3YNedxBGFjU">
    
    <link rel="preload" href="/assets/css/0.styles.b2187a0e.css" as="style"><link rel="preload" href="/assets/js/app.0cf0d483.js" as="script"><link rel="preload" href="/assets/js/2.d537feee.js" as="script"><link rel="preload" href="/assets/js/97.af175cfb.js" as="script"><link rel="prefetch" href="/assets/js/10.000b9996.js"><link rel="prefetch" href="/assets/js/100.01b72e43.js"><link rel="prefetch" href="/assets/js/101.ab99304f.js"><link rel="prefetch" href="/assets/js/102.b1ad1ca3.js"><link rel="prefetch" href="/assets/js/11.0ec32f4a.js"><link rel="prefetch" href="/assets/js/12.7ad41a4b.js"><link rel="prefetch" href="/assets/js/13.40c87056.js"><link rel="prefetch" href="/assets/js/14.2d8bcb8d.js"><link rel="prefetch" href="/assets/js/15.a7295b50.js"><link rel="prefetch" href="/assets/js/16.7158147a.js"><link rel="prefetch" href="/assets/js/17.9215c05c.js"><link rel="prefetch" href="/assets/js/18.582ac004.js"><link rel="prefetch" href="/assets/js/19.7cc8fc31.js"><link rel="prefetch" href="/assets/js/20.1b1e2af4.js"><link rel="prefetch" href="/assets/js/21.b2dadf61.js"><link rel="prefetch" href="/assets/js/22.b19748fc.js"><link rel="prefetch" href="/assets/js/23.f2db4fa1.js"><link rel="prefetch" href="/assets/js/24.3c31aece.js"><link rel="prefetch" href="/assets/js/25.531fd9fb.js"><link rel="prefetch" href="/assets/js/26.6e841782.js"><link rel="prefetch" href="/assets/js/27.33add335.js"><link rel="prefetch" href="/assets/js/28.35498a04.js"><link rel="prefetch" href="/assets/js/29.aabed81c.js"><link rel="prefetch" href="/assets/js/3.0a121536.js"><link rel="prefetch" href="/assets/js/30.54f44e16.js"><link rel="prefetch" href="/assets/js/31.8ecbe721.js"><link rel="prefetch" href="/assets/js/32.873f263a.js"><link rel="prefetch" href="/assets/js/33.106c15c8.js"><link rel="prefetch" href="/assets/js/34.349d6fdb.js"><link rel="prefetch" href="/assets/js/35.73569c69.js"><link rel="prefetch" href="/assets/js/36.24f08107.js"><link rel="prefetch" href="/assets/js/37.da460b69.js"><link rel="prefetch" href="/assets/js/38.2f6c4646.js"><link rel="prefetch" href="/assets/js/39.435f8032.js"><link rel="prefetch" href="/assets/js/4.972cd905.js"><link rel="prefetch" href="/assets/js/40.4760c47b.js"><link rel="prefetch" href="/assets/js/41.a7cc5e5d.js"><link rel="prefetch" href="/assets/js/42.fc3e47f2.js"><link rel="prefetch" href="/assets/js/43.a13c67d4.js"><link rel="prefetch" href="/assets/js/44.46c317cb.js"><link rel="prefetch" href="/assets/js/45.f11cdfc6.js"><link rel="prefetch" href="/assets/js/46.541bd27d.js"><link rel="prefetch" href="/assets/js/47.5705cba6.js"><link rel="prefetch" href="/assets/js/48.d2040c65.js"><link rel="prefetch" href="/assets/js/49.cbb5c42b.js"><link rel="prefetch" href="/assets/js/5.e2cb1c20.js"><link rel="prefetch" href="/assets/js/50.39732f79.js"><link rel="prefetch" href="/assets/js/51.99b82adb.js"><link rel="prefetch" href="/assets/js/52.808c9ece.js"><link rel="prefetch" href="/assets/js/53.67e417da.js"><link rel="prefetch" href="/assets/js/54.1c722ed6.js"><link rel="prefetch" href="/assets/js/55.afc1aceb.js"><link rel="prefetch" href="/assets/js/56.a463c15a.js"><link rel="prefetch" href="/assets/js/57.f90cb51c.js"><link rel="prefetch" href="/assets/js/58.8e11cf82.js"><link rel="prefetch" href="/assets/js/59.ed4fc789.js"><link rel="prefetch" href="/assets/js/6.59e5684c.js"><link rel="prefetch" href="/assets/js/60.adcefa1d.js"><link rel="prefetch" href="/assets/js/61.a6446644.js"><link rel="prefetch" href="/assets/js/62.1721a24b.js"><link rel="prefetch" href="/assets/js/63.726a6135.js"><link rel="prefetch" href="/assets/js/64.e2db755f.js"><link rel="prefetch" href="/assets/js/65.e92b9d46.js"><link rel="prefetch" href="/assets/js/66.b4d3e87e.js"><link rel="prefetch" href="/assets/js/67.5f3eaf29.js"><link rel="prefetch" href="/assets/js/68.85b4615c.js"><link rel="prefetch" href="/assets/js/69.63d409f1.js"><link rel="prefetch" href="/assets/js/7.b3ddb4c6.js"><link rel="prefetch" href="/assets/js/70.6f3f7332.js"><link rel="prefetch" href="/assets/js/71.0a087c8a.js"><link rel="prefetch" href="/assets/js/72.6513570f.js"><link rel="prefetch" href="/assets/js/73.896f4d9c.js"><link rel="prefetch" href="/assets/js/74.ecc3d286.js"><link rel="prefetch" href="/assets/js/75.b06a8964.js"><link rel="prefetch" href="/assets/js/76.3032add1.js"><link rel="prefetch" href="/assets/js/77.4fb4de51.js"><link rel="prefetch" href="/assets/js/78.42cf5412.js"><link rel="prefetch" href="/assets/js/79.0ba88830.js"><link rel="prefetch" href="/assets/js/8.bb97c23d.js"><link rel="prefetch" href="/assets/js/80.aaa6656c.js"><link rel="prefetch" href="/assets/js/81.4a1804f1.js"><link rel="prefetch" href="/assets/js/82.7ad53f0e.js"><link rel="prefetch" href="/assets/js/83.c4173111.js"><link rel="prefetch" href="/assets/js/84.cb7300ea.js"><link rel="prefetch" href="/assets/js/85.81c0ea84.js"><link rel="prefetch" href="/assets/js/86.317fcc1c.js"><link rel="prefetch" href="/assets/js/87.6f50a42f.js"><link rel="prefetch" href="/assets/js/88.898a4cd8.js"><link rel="prefetch" href="/assets/js/89.ddbe5c5c.js"><link rel="prefetch" href="/assets/js/9.596e7f45.js"><link rel="prefetch" href="/assets/js/90.c1e40160.js"><link rel="prefetch" href="/assets/js/91.f85fdec8.js"><link rel="prefetch" href="/assets/js/92.1d7822a2.js"><link rel="prefetch" href="/assets/js/93.4ee2ef21.js"><link rel="prefetch" href="/assets/js/94.3760a5b4.js"><link rel="prefetch" href="/assets/js/95.b5be500a.js"><link rel="prefetch" href="/assets/js/96.e570f48e.js"><link rel="prefetch" href="/assets/js/98.ca3ecf81.js"><link rel="prefetch" href="/assets/js/99.2fc27711.js">
    <link rel="stylesheet" href="/assets/css/0.styles.b2187a0e.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><!----> <span class="site-name">工具箱的深度学习记事簿</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="https://github.com/VisualDust/ml.akasaki.space" target="_blank" rel="noopener noreferrer" class="nav-link external">
  View on Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div><div class="nav-item"><a href="https://github.com/VisualDust" target="_blank" rel="noopener noreferrer" class="nav-link external">
  工具箱
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="https://github.com/VisualDust/ml.akasaki.space" target="_blank" rel="noopener noreferrer" class="nav-link external">
  View on Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div><div class="nav-item"><a href="https://github.com/VisualDust" target="_blank" rel="noopener noreferrer" class="nav-link external">
  工具箱
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第零章：在开始之前</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第一章上：HelloWorld</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第一章下：深度学习基础</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第二章上：卷积神经网络</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第二章下：经典卷积神经网络</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第三章上：谈一些计算机视觉方向</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第三章下：了解更高级的技术</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>魔法部日志（又名论文阅读日志）</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/unlimited-paper-works/[0]unlimited-paper-works.html" class="sidebar-link">欢迎来到魔法部日志</a></li><li><a href="/unlimited-paper-works/[1]The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs.html" class="sidebar-link">The Devil is in the Decoder: Classification, Regression and GANs</a></li><li><a href="/unlimited-paper-works/[2]Threat-of-Adversarial-Attacks-on-Deep-Learning-in-Computer-Vision-A-Survey.html" class="sidebar-link">Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey</a></li><li><a href="/unlimited-paper-works/[3]Progressive-Semantic-Segmentation.html" class="sidebar-link">Progressive Semantic Segmentation</a></li><li><a href="/unlimited-paper-works/[4]Decoders-Matter-for-Semantic-Segmentation-Data-Dependent-Decoding-Enables-Flexible-Feature-Aggregation.html" class="sidebar-link">Decoders Matter for Semantic Segmentation: Data-Dependent Decoding Enables Flexible Feature Aggregation</a></li><li><a href="/unlimited-paper-works/[5]HLA-Face-Joint-High-Low-Adaptation-for-Low-Light-Face-Detection.html" class="sidebar-link">HLA-Face Joint High-Low Adaptation for Low Light Face Detection</a></li><li><a href="/unlimited-paper-works/[6]DeepLab-Series.html" class="sidebar-link">DeepLab Series</a></li><li><a href="/unlimited-paper-works/[7]Cross-Dataset-Collaborative-Learning-for-Semantic-Segmentation.html" class="sidebar-link">Cross-Dataset Collaborative Learning for Semantic Segmentation</a></li><li><a href="/unlimited-paper-works/[8]Dynamic-Neural-Networks-A-Survey.html" class="sidebar-link">Dynamic Neural Networks: A Survey</a></li><li><a href="/unlimited-paper-works/[9]Feature-Pyramid-Networks-for-Object-Detection.html" class="sidebar-link">Feature Pyramid Networks for Object Detection</a></li><li><a href="/unlimited-paper-works/[10]Overview-Of-Semantic-Segmentation.html" class="sidebar-link">A Review on Deep Learning Techniques Applied to Semantic Segmentation</a></li><li><a href="/unlimited-paper-works/[11]Image-Segmentation-Using-Deep-Learning-A-Survey.html" class="sidebar-link">Image Segmentation Using Deep Learning: A Survey</a></li><li><a href="/unlimited-paper-works/[12]MobileNetV2-Inverted-Residuals-and-Linear-bottleneck.html" class="sidebar-link">MobileNetV2: Inverted Residuals and Linear Bottlenecks</a></li><li><a href="/unlimited-paper-works/[13]Fast-SCNN-Fast-Semantic-Segmentation-Network.html" class="sidebar-link">Fast-SCNN: Fast Semantic Segmentation Network</a></li><li><a href="/unlimited-paper-works/[14]MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications.html" class="sidebar-link">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</a></li><li><a href="/unlimited-paper-works/[15]Gated-Channel-Transformation-for-Visual-Recognition.html" class="sidebar-link">Gated Channel Transformation for Visual Recognition</a></li><li><a href="/unlimited-paper-works/[16]Convolutional-Block-Attention-Module.html" class="active sidebar-link">Convolutional Block Attention Module</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[16]Convolutional-Block-Attention-Module.html#摘要" class="sidebar-link">摘要</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[16]Convolutional-Block-Attention-Module.html#介绍" class="sidebar-link">介绍</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[16]Convolutional-Block-Attention-Module.html#注意力机制" class="sidebar-link">注意力机制</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[16]Convolutional-Block-Attention-Module.html#cbam" class="sidebar-link">CBAM</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[16]Convolutional-Block-Attention-Module.html#整体结构" class="sidebar-link">整体结构</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[16]Convolutional-Block-Attention-Module.html#channel-attention-module" class="sidebar-link">Channel attention module</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[16]Convolutional-Block-Attention-Module.html#spatial-attention-module" class="sidebar-link">Spatial attention module</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[16]Convolutional-Block-Attention-Module.html#arrangement-of-attention-modules" class="sidebar-link">Arrangement of attention modules.</a></li></ul></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[16]Convolutional-Block-Attention-Module.html#ablation-studies-消融研究" class="sidebar-link">Ablation studies（消融研究）</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[16]Convolutional-Block-Attention-Module.html#channel-attention" class="sidebar-link">Channel attention</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[16]Convolutional-Block-Attention-Module.html#spatial-attention" class="sidebar-link">Spatial attention</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[16]Convolutional-Block-Attention-Module.html#arrangement-of-the-channel-and-spatial-attention" class="sidebar-link">Arrangement of the channel and spatial attention.</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[16]Convolutional-Block-Attention-Module.html#最终效果" class="sidebar-link">最终效果</a></li></ul></li></ul></li><li><a href="/unlimited-paper-works/[17]Boundary-IoU-Improving-Object-Centri Image-Segmentation-Evaluation.html" class="sidebar-link">Boundary IoU: Improving Object-Centric Image Segmentation Evaluation</a></li><li><a href="/unlimited-paper-works/[18]Involution-Inverting-the-Inherence-of-Convolution-for-Visual-Recognition.html" class="sidebar-link">Involution: Inverting the Inherence of Convolution for Visual Recognition</a></li><li><a href="/unlimited-paper-works/[19]PointRend-Image-Segmentation-as-Rendering.html" class="sidebar-link">PointRend: Image Segmentation as Rendering</a></li><li><a href="/unlimited-paper-works/[20]Transformer-Attention-is-all-you-need.html" class="sidebar-link">Transformer: Attention is all you need</a></li><li><a href="/unlimited-paper-works/[21]RefineMask_Towards_High-Quality_Instance_Segmentationwith_Fine-Grained_Features.html" class="sidebar-link">RefineMask: Towards High-Quality Instance Segmentationwith Fine-Grained Features</a></li><li><a href="/unlimited-paper-works/[22]GLADNet-Low-Light-Enhancement-Network-with-Global-Awareness.html" class="sidebar-link">GLADNet: Low-Light Enhancement Network with Global Awareness</a></li><li><a href="/unlimited-paper-works/[23]Squeeze-and-Excitation-Networks.html" class="sidebar-link">Squeeze-and-Excitation Networks (SENet)</a></li><li><a href="/unlimited-paper-works/[24]BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation.html" class="sidebar-link">BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation</a></li><li><a href="/unlimited-paper-works/[25]Rethinking-BiSeNet-For-Real-time-Semantic-Segmentation.html" class="sidebar-link">Rethinking BiSeNet For Real-time Semantic Segmentation</a></li><li><a href="/unlimited-paper-works/[26]CBAM-Convolutional-Block-Attention-Module.html" class="sidebar-link">CBAM: Convolutional Block Attention Module</a></li><li><a href="/unlimited-paper-works/[27]Non-local-Neural-Networks.html" class="sidebar-link">Non-local Neural Networks</a></li><li><a href="/unlimited-paper-works/[28]GCNet-Non-local-Networks-Meet-Squeeze-Excitation-Networks-and-Beyond.html" class="sidebar-link">GCNet: Global Context Networks (Non-local Networks Meet Squeeze-Excitation Networks and Beyond)</a></li><li><a href="/unlimited-paper-works/[29]Disentangled-Non-Local-Neural-Networks.html" class="sidebar-link">Disentangled Non-Local Neural Networks</a></li><li><a href="/unlimited-paper-works/[30]RetinexNet-for-Low-Light-Enhancement.html" class="sidebar-link">Deep Retinex Decomposition for Low-Light Enhancement</a></li><li><a href="/unlimited-paper-works/[31]MSR-netLow-light-Image-Enhancement-Using-Deep-Convolutional-Network.html" class="sidebar-link">MSR-net:Low-light Image Enhancement Using Deep Convolutional Network</a></li><li><a href="/unlimited-paper-works/[32]LLCNN-A-Convolutional-Neural-Network-for-Low-light-Image-Enhancement.html" class="sidebar-link">LLCNN: A convolutional neural network for low-light image enhancement</a></li></ul></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>附录1：好朋友们</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>附录2：数学是真正的圣经</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>附录3：信号和采样的学问（DSP）</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>附录4：TensorFlow编程策略</span> <span class="arrow right"></span></p> <!----></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="convolutional-block-attention-module"><a href="#convolutional-block-attention-module" class="header-anchor">#</a> Convolutional Block Attention Module</h1> <h3 id="这篇笔记的写作者是asthestarsfall。"><a href="#这篇笔记的写作者是asthestarsfall。" class="header-anchor">#</a> 这篇笔记的写作者是<a href="https://github.com/asthestarsfalll" target="_blank" rel="noopener noreferrer">AsTheStarsFall<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>。</h3> <blockquote><p>论文名称：<a href="https://arxiv.org/abs/1807.06521" target="_blank" rel="noopener noreferrer">CBAM: Convolutional Block Attention Module<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p>作者：Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon，Korea Advanced Institute of Science and Technology, Daejeon, Korea</p></blockquote> <h2 id="摘要"><a href="#摘要" class="header-anchor">#</a> 摘要</h2> <ul><li>CBAM（Convolutional Block Attention Moudule)是一种简单有效的<a href="https://www.cnblogs.com/samshare/p/11801806.html" target="_blank" rel="noopener noreferrer">前馈<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>卷积神经网络注意力模块。</li> <li>该模块为混合域注意力机制（）从通道和空间两个方面依次推断attention map。</li> <li>CBAM是一个轻量级的通用模块，可以无缝集成到任何CNN中。</li></ul> <p><strong>关键词:物体识别，注意机制，门控卷积</strong></p> <h2 id="介绍"><a href="#介绍" class="header-anchor">#</a> 介绍</h2> <ul><li>卷积神经网络(CNNs)基于其丰富的表达能力显著提高了视觉任务的性能，目前的主要关注网络的三个重要因素：<strong>深度，宽度和基数</strong>（Cardinality）。</li> <li>从LeNet到残差网络，网络变的更加深入，表达形式更加丰富；GoogLeNet表明宽度是提高模型性能的另一个重要因素；Xception和ResNext则通过增加网络的<strong>基数</strong>，在节省参数的同时，来获得比深度、宽度更强的表达能力（引用于ResNext论文）。</li> <li>除了这些因素之外，本文考察了与网络结构设计不同的方面——注意力。</li></ul> <h2 id="注意力机制"><a href="#注意力机制" class="header-anchor">#</a> 注意力机制</h2> <ul><li>注意（attention）在人类感知中起着重要的作用。人类视觉系统的一个重要特性是，它不会试图一次性处理整个场景，而是利用一系列的局部一瞥（a sequence of partial glimpses）来获得对显著部分的关注。</li> <li>近年来有一些尝试将注意力机制加入CNN中，如使用Endcoder-Decoder结构注意力模块的Residual Attention Network，使用“Squeeze-Excitation“模块的SEnet。</li> <li>具体可见<a href="https://asthestarsfalll.icu/2021/05/12/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/" target="_blank" rel="noopener noreferrer">注意力机制<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul> <h2 id="cbam"><a href="#cbam" class="header-anchor">#</a> CBAM</h2> <h3 id="整体结构"><a href="#整体结构" class="header-anchor">#</a> 整体结构</h3> <p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20210512124618161.png" alt="image-20210512124618161"></p> <ul><li><p>CBAM在混合域（通道域、空间域）上引入注意力机制，拥有更强的表达能力；</p></li> <li><p>整个过程可以被概括为：</p> <p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>F</mi><mtext>’</mtext><mo>=</mo><msub><mi>M</mi><mi>c</mi></msub><mo stretchy="false">(</mo><mi>F</mi><mo stretchy="false">)</mo><mo>⊗</mo><mi>F</mi><mspace linebreak="newline"></mspace><msup><mi>F</mi><mrow><mo mathvariant="normal">′</mo><mo mathvariant="normal">′</mo></mrow></msup><mo>=</mo><msub><mi>M</mi><mi>s</mi></msub><mo stretchy="false">(</mo><msup><mi>F</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo><mo>⊗</mo><msup><mi>F</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">F’=M_c(F)\otimes F\\
F''=M_s(F')\otimes F'
</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mord">’</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⊗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.801892em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.051892em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⊗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.801892em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span></p> <p>其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi></mrow><annotation encoding="application/x-tex">F</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span></span></span></span>为模块的输入，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>M</mi><mi>c</mi></msub><mtext>、</mtext><msub><mi>M</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">M_c、M_s</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord cjk_fallback">、</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>表示通道注意力图和空间注意力图，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>⊗</mo></mrow><annotation encoding="application/x-tex">\otimes</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord">⊗</span></span></span></span>表示element-wise multiply，在具体的实现过程中会相应的进行广播。</p></li></ul> <h3 id="channel-attention-module"><a href="#channel-attention-module" class="header-anchor">#</a> Channel attention module</h3> <p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20210512130136918.png" alt="image-20210512130136918"></p> <p>利用通道之间的关系生成通道注意图（channel attention map），具体可见<a href="https://asthestarsfalll.icu/2021/05/12/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/" target="_blank" rel="noopener noreferrer">注意力机制<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p>通道注意力主要关注的是图像的“什么”更有意义。</p> <p>实现过程：</p> <ol><li>对input进行<strong>全局平均池化</strong>和<strong>全局最大池化</strong>来聚集空间内的信息;</li> <li>通过一个<strong>共享的</strong>MLP(多层感知机)(具体实现可用1X1卷积层),为了减少参数隐藏层的通道数被设置为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mi>C</mi><mi>R</mi></mfrac></mrow><annotation encoding="application/x-tex">\frac{C}{R}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:1.217331em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.872331em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.00773em;">R</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">C</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>，在第一层之后设置了ReLU函数来引入非线性（类似于SENet，这种结构出现在各种网络之中，作用之一是为了减少参数和计算量，作用之二是为了获得更多的非线性）；</li> <li>对应求和之后经过一个ReLU层得到最终的Channel attention map</li> <li>将其与input相乘（会自动进行广播）。</li></ol> <p>代码复现：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">Channel_module</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>  in_ch<span class="token punctuation">,</span> ratio<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Channel_module<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>global_avg <span class="token operator">=</span> nn<span class="token punctuation">.</span>AdaptiveAvgPool2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>global_max <span class="token operator">=</span> nn<span class="token punctuation">.</span>AdaptiveMaxPool2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_ch<span class="token punctuation">,</span> in_ch<span class="token operator">//</span>ratio<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>relu <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_ch<span class="token operator">//</span>ratio<span class="token punctuation">,</span> in_ch<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        a <span class="token operator">=</span> self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>self<span class="token punctuation">.</span>global_avg<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        m <span class="token operator">=</span> self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>self<span class="token punctuation">.</span>global_avg<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        attention <span class="token operator">=</span> F<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>a<span class="token operator">+</span>m<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x<span class="token operator">*</span>attention
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br></div></div><h3 id="spatial-attention-module"><a href="#spatial-attention-module" class="header-anchor">#</a> Spatial attention module</h3> <p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20210512133601683.png" alt="image-20210512133601683"></p> <p>利用空间之间的关系来生成空间注意力图（spatial attention map）</p> <p>空间注意力主要关注“哪里”有重要信息，与通道注意力相辅相成。</p> <p>实现过程：</p> <ol><li>在通道维度上分别进行平均池化和最大池化，然后进行concat；</li> <li>经过一个7X7的卷积层，将通道数降为1；</li> <li>Sigmoid函数；</li> <li>与inputs也就是上一层的Channel-refined feature对应相乘。</li></ol> <p>代码复现：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">Spatial_module</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Spatial_module<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>
                              bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>  <span class="token comment"># 使用padding保持大小不变</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        a <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>x<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>  <span class="token comment"># 沿着channel维度计算均值和最大值</span>
        m<span class="token punctuation">,</span> _ <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        cx <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>a<span class="token punctuation">,</span> m<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        cx <span class="token operator">=</span> F<span class="token punctuation">.</span>sigmiod<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv<span class="token punctuation">(</span>cx<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> cx<span class="token operator">*</span>x
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br></div></div><h3 id="arrangement-of-attention-modules"><a href="#arrangement-of-attention-modules" class="header-anchor">#</a> Arrangement of attention modules.</h3> <p>以上两个注意力模块计算互补的注意力，考虑到这一点，这两个模块可以并行或是顺序排列。实验表明，顺序排列比并行排列效果更好，其中通道优先顺序略好于空间优先顺序。</p> <h2 id="ablation-studies-消融研究"><a href="#ablation-studies-消融研究" class="header-anchor">#</a> Ablation studies（消融研究）</h2> <p>作者团队首先寻找计算通道注意的有效方法，然后是空间注意。最后，我们考虑如何结合通道关注模块和空间关注模块。</p> <h3 id="channel-attention"><a href="#channel-attention" class="header-anchor">#</a> Channel attention</h3> <p>作者团队比较了3种不同的通道注意力:平均池化、最大池化和两种池化的联合使用。</p> <p><img src="https://i.loli.net/2021/05/12/kFItTg6yD1u5joJ.png" alt="image-20210512160839407"></p> <p>可以看到，最大池化与平均池化同样重要，而SENet忽略了最大池化的重要性。</p> <p><strong>对显著部分进行编码的最大池化特征可以补偿对全局信息软编码的平均池化特征</strong>。</p> <p>在空间注意力的研究当中，将直接使用最大池化特征和平均池化特征，并将R设置为16。</p> <h3 id="spatial-attention"><a href="#spatial-attention" class="header-anchor">#</a> Spatial attention</h3> <p>作者团队考虑了两种空间注意力的方案：<strong>一是使用通道维度上的平均池化和最大池化，二是使用1X1卷积进行降维</strong>。此外还研究了3X3和7X7卷积核的影响。在实验当中，将空间注意力模块置于通道注意力模块之后。</p> <p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20210512162421957.png" alt="image-20210512162421957"></p> <p>可以看到，通道池化的效果更好，同时，使用较大的核会产生更好的精度，这意味着需要一个更大的感受野来决定空间上的重要区域。</p> <h3 id="arrangement-of-the-channel-and-spatial-attention"><a href="#arrangement-of-the-channel-and-spatial-attention" class="header-anchor">#</a> Arrangement of the channel and spatial attention.</h3> <p>作者团队考虑了三种不同的模块安排方案：通道优先，空间优先和并行。</p> <p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20210512163932604.png" alt="image-20210512163932604"></p> <p>可以看到，通道优先的效果更好。</p> <h3 id="最终效果"><a href="#最终效果" class="header-anchor">#</a> 最终效果</h3> <p><img src="https://gitee.com/Thedeadleaf/images/raw/master/image-20210512164218849.png" alt="image-20210512164218849"></p></div> <footer class="page-edit"><!----> <div class="last-updated"><span class="prefix">上次更新时间:</span> <span class="time">2021/7/27 上午4:27:40</span></div></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/unlimited-paper-works/[15]Gated-Channel-Transformation-for-Visual-Recognition.html" class="prev">
        Gated Channel Transformation for Visual Recognition
      </a></span> <span class="next"><a href="/unlimited-paper-works/[17]Boundary-IoU-Improving-Object-Centri Image-Segmentation-Evaluation.html">
        Boundary IoU: Improving Object-Centric Image Segmentation Evaluation
      </a>
      →
    </span></p></div> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.0cf0d483.js" defer></script><script src="/assets/js/2.d537feee.js" defer></script><script src="/assets/js/97.af175cfb.js" defer></script>
  </body>
</html>
