<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Boundary IoU: Improving Object-Centric Image Segmentation Evaluation | 工具箱的深度学习记事簿</title>
    <meta name="generator" content="VuePress 1.8.2">
    <link rel="icon" href="/statics/logo.svg">
    <meta name="description" content="这里包含了我从入门到依然在入门的过程中接触到的大部分知识。翻翻目录，也许能找到有用的">
    <meta name="keywords" content="Akasaki,Deep learning,Machine learning,工具箱,工具箱的深度学习记事簿,Akasaki的深度学习记事簿">
    <meta name="google-site-verification" content="VVNYs0bXM_EKTgxJ8XIfvXShjHsksGNv3YNedxBGFjU">
    
    <link rel="preload" href="/assets/css/0.styles.b2187a0e.css" as="style"><link rel="preload" href="/assets/js/app.0cf0d483.js" as="script"><link rel="preload" href="/assets/js/2.d537feee.js" as="script"><link rel="preload" href="/assets/js/98.ca3ecf81.js" as="script"><link rel="prefetch" href="/assets/js/10.000b9996.js"><link rel="prefetch" href="/assets/js/100.01b72e43.js"><link rel="prefetch" href="/assets/js/101.ab99304f.js"><link rel="prefetch" href="/assets/js/102.b1ad1ca3.js"><link rel="prefetch" href="/assets/js/11.0ec32f4a.js"><link rel="prefetch" href="/assets/js/12.7ad41a4b.js"><link rel="prefetch" href="/assets/js/13.40c87056.js"><link rel="prefetch" href="/assets/js/14.2d8bcb8d.js"><link rel="prefetch" href="/assets/js/15.a7295b50.js"><link rel="prefetch" href="/assets/js/16.7158147a.js"><link rel="prefetch" href="/assets/js/17.9215c05c.js"><link rel="prefetch" href="/assets/js/18.582ac004.js"><link rel="prefetch" href="/assets/js/19.7cc8fc31.js"><link rel="prefetch" href="/assets/js/20.1b1e2af4.js"><link rel="prefetch" href="/assets/js/21.b2dadf61.js"><link rel="prefetch" href="/assets/js/22.b19748fc.js"><link rel="prefetch" href="/assets/js/23.f2db4fa1.js"><link rel="prefetch" href="/assets/js/24.3c31aece.js"><link rel="prefetch" href="/assets/js/25.531fd9fb.js"><link rel="prefetch" href="/assets/js/26.6e841782.js"><link rel="prefetch" href="/assets/js/27.33add335.js"><link rel="prefetch" href="/assets/js/28.35498a04.js"><link rel="prefetch" href="/assets/js/29.aabed81c.js"><link rel="prefetch" href="/assets/js/3.0a121536.js"><link rel="prefetch" href="/assets/js/30.54f44e16.js"><link rel="prefetch" href="/assets/js/31.8ecbe721.js"><link rel="prefetch" href="/assets/js/32.873f263a.js"><link rel="prefetch" href="/assets/js/33.106c15c8.js"><link rel="prefetch" href="/assets/js/34.349d6fdb.js"><link rel="prefetch" href="/assets/js/35.73569c69.js"><link rel="prefetch" href="/assets/js/36.24f08107.js"><link rel="prefetch" href="/assets/js/37.da460b69.js"><link rel="prefetch" href="/assets/js/38.2f6c4646.js"><link rel="prefetch" href="/assets/js/39.435f8032.js"><link rel="prefetch" href="/assets/js/4.972cd905.js"><link rel="prefetch" href="/assets/js/40.4760c47b.js"><link rel="prefetch" href="/assets/js/41.a7cc5e5d.js"><link rel="prefetch" href="/assets/js/42.fc3e47f2.js"><link rel="prefetch" href="/assets/js/43.a13c67d4.js"><link rel="prefetch" href="/assets/js/44.46c317cb.js"><link rel="prefetch" href="/assets/js/45.f11cdfc6.js"><link rel="prefetch" href="/assets/js/46.541bd27d.js"><link rel="prefetch" href="/assets/js/47.5705cba6.js"><link rel="prefetch" href="/assets/js/48.d2040c65.js"><link rel="prefetch" href="/assets/js/49.cbb5c42b.js"><link rel="prefetch" href="/assets/js/5.e2cb1c20.js"><link rel="prefetch" href="/assets/js/50.39732f79.js"><link rel="prefetch" href="/assets/js/51.99b82adb.js"><link rel="prefetch" href="/assets/js/52.808c9ece.js"><link rel="prefetch" href="/assets/js/53.67e417da.js"><link rel="prefetch" href="/assets/js/54.1c722ed6.js"><link rel="prefetch" href="/assets/js/55.afc1aceb.js"><link rel="prefetch" href="/assets/js/56.a463c15a.js"><link rel="prefetch" href="/assets/js/57.f90cb51c.js"><link rel="prefetch" href="/assets/js/58.8e11cf82.js"><link rel="prefetch" href="/assets/js/59.ed4fc789.js"><link rel="prefetch" href="/assets/js/6.59e5684c.js"><link rel="prefetch" href="/assets/js/60.adcefa1d.js"><link rel="prefetch" href="/assets/js/61.a6446644.js"><link rel="prefetch" href="/assets/js/62.1721a24b.js"><link rel="prefetch" href="/assets/js/63.726a6135.js"><link rel="prefetch" href="/assets/js/64.e2db755f.js"><link rel="prefetch" href="/assets/js/65.e92b9d46.js"><link rel="prefetch" href="/assets/js/66.b4d3e87e.js"><link rel="prefetch" href="/assets/js/67.5f3eaf29.js"><link rel="prefetch" href="/assets/js/68.85b4615c.js"><link rel="prefetch" href="/assets/js/69.63d409f1.js"><link rel="prefetch" href="/assets/js/7.b3ddb4c6.js"><link rel="prefetch" href="/assets/js/70.6f3f7332.js"><link rel="prefetch" href="/assets/js/71.0a087c8a.js"><link rel="prefetch" href="/assets/js/72.6513570f.js"><link rel="prefetch" href="/assets/js/73.896f4d9c.js"><link rel="prefetch" href="/assets/js/74.ecc3d286.js"><link rel="prefetch" href="/assets/js/75.b06a8964.js"><link rel="prefetch" href="/assets/js/76.3032add1.js"><link rel="prefetch" href="/assets/js/77.4fb4de51.js"><link rel="prefetch" href="/assets/js/78.42cf5412.js"><link rel="prefetch" href="/assets/js/79.0ba88830.js"><link rel="prefetch" href="/assets/js/8.bb97c23d.js"><link rel="prefetch" href="/assets/js/80.aaa6656c.js"><link rel="prefetch" href="/assets/js/81.4a1804f1.js"><link rel="prefetch" href="/assets/js/82.7ad53f0e.js"><link rel="prefetch" href="/assets/js/83.c4173111.js"><link rel="prefetch" href="/assets/js/84.cb7300ea.js"><link rel="prefetch" href="/assets/js/85.81c0ea84.js"><link rel="prefetch" href="/assets/js/86.317fcc1c.js"><link rel="prefetch" href="/assets/js/87.6f50a42f.js"><link rel="prefetch" href="/assets/js/88.898a4cd8.js"><link rel="prefetch" href="/assets/js/89.ddbe5c5c.js"><link rel="prefetch" href="/assets/js/9.596e7f45.js"><link rel="prefetch" href="/assets/js/90.c1e40160.js"><link rel="prefetch" href="/assets/js/91.f85fdec8.js"><link rel="prefetch" href="/assets/js/92.1d7822a2.js"><link rel="prefetch" href="/assets/js/93.4ee2ef21.js"><link rel="prefetch" href="/assets/js/94.3760a5b4.js"><link rel="prefetch" href="/assets/js/95.b5be500a.js"><link rel="prefetch" href="/assets/js/96.e570f48e.js"><link rel="prefetch" href="/assets/js/97.af175cfb.js"><link rel="prefetch" href="/assets/js/99.2fc27711.js">
    <link rel="stylesheet" href="/assets/css/0.styles.b2187a0e.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><!----> <span class="site-name">工具箱的深度学习记事簿</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="https://github.com/VisualDust/ml.akasaki.space" target="_blank" rel="noopener noreferrer" class="nav-link external">
  View on Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div><div class="nav-item"><a href="https://github.com/VisualDust" target="_blank" rel="noopener noreferrer" class="nav-link external">
  工具箱
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="https://github.com/VisualDust/ml.akasaki.space" target="_blank" rel="noopener noreferrer" class="nav-link external">
  View on Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div><div class="nav-item"><a href="https://github.com/VisualDust" target="_blank" rel="noopener noreferrer" class="nav-link external">
  工具箱
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第零章：在开始之前</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第一章上：HelloWorld</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第一章下：深度学习基础</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第二章上：卷积神经网络</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第二章下：经典卷积神经网络</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第三章上：谈一些计算机视觉方向</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第三章下：了解更高级的技术</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>魔法部日志（又名论文阅读日志）</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/unlimited-paper-works/[0]unlimited-paper-works.html" class="sidebar-link">欢迎来到魔法部日志</a></li><li><a href="/unlimited-paper-works/[1]The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs.html" class="sidebar-link">The Devil is in the Decoder: Classification, Regression and GANs</a></li><li><a href="/unlimited-paper-works/[2]Threat-of-Adversarial-Attacks-on-Deep-Learning-in-Computer-Vision-A-Survey.html" class="sidebar-link">Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey</a></li><li><a href="/unlimited-paper-works/[3]Progressive-Semantic-Segmentation.html" class="sidebar-link">Progressive Semantic Segmentation</a></li><li><a href="/unlimited-paper-works/[4]Decoders-Matter-for-Semantic-Segmentation-Data-Dependent-Decoding-Enables-Flexible-Feature-Aggregation.html" class="sidebar-link">Decoders Matter for Semantic Segmentation: Data-Dependent Decoding Enables Flexible Feature Aggregation</a></li><li><a href="/unlimited-paper-works/[5]HLA-Face-Joint-High-Low-Adaptation-for-Low-Light-Face-Detection.html" class="sidebar-link">HLA-Face Joint High-Low Adaptation for Low Light Face Detection</a></li><li><a href="/unlimited-paper-works/[6]DeepLab-Series.html" class="sidebar-link">DeepLab Series</a></li><li><a href="/unlimited-paper-works/[7]Cross-Dataset-Collaborative-Learning-for-Semantic-Segmentation.html" class="sidebar-link">Cross-Dataset Collaborative Learning for Semantic Segmentation</a></li><li><a href="/unlimited-paper-works/[8]Dynamic-Neural-Networks-A-Survey.html" class="sidebar-link">Dynamic Neural Networks: A Survey</a></li><li><a href="/unlimited-paper-works/[9]Feature-Pyramid-Networks-for-Object-Detection.html" class="sidebar-link">Feature Pyramid Networks for Object Detection</a></li><li><a href="/unlimited-paper-works/[10]Overview-Of-Semantic-Segmentation.html" class="sidebar-link">A Review on Deep Learning Techniques Applied to Semantic Segmentation</a></li><li><a href="/unlimited-paper-works/[11]Image-Segmentation-Using-Deep-Learning-A-Survey.html" class="sidebar-link">Image Segmentation Using Deep Learning: A Survey</a></li><li><a href="/unlimited-paper-works/[12]MobileNetV2-Inverted-Residuals-and-Linear-bottleneck.html" class="sidebar-link">MobileNetV2: Inverted Residuals and Linear Bottlenecks</a></li><li><a href="/unlimited-paper-works/[13]Fast-SCNN-Fast-Semantic-Segmentation-Network.html" class="sidebar-link">Fast-SCNN: Fast Semantic Segmentation Network</a></li><li><a href="/unlimited-paper-works/[14]MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications.html" class="sidebar-link">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</a></li><li><a href="/unlimited-paper-works/[15]Gated-Channel-Transformation-for-Visual-Recognition.html" class="sidebar-link">Gated Channel Transformation for Visual Recognition</a></li><li><a href="/unlimited-paper-works/[16]Convolutional-Block-Attention-Module.html" class="sidebar-link">Convolutional Block Attention Module</a></li><li><a href="/unlimited-paper-works/[17]Boundary-IoU-Improving-Object-Centri Image-Segmentation-Evaluation.html" class="active sidebar-link">Boundary IoU: Improving Object-Centric Image Segmentation Evaluation</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[17]Boundary-IoU-Improving-Object-Centri Image-Segmentation-Evaluation.html#mask-iou和pixel-accuracy" class="sidebar-link">Mask IoU和Pixel Accuracy</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[17]Boundary-IoU-Improving-Object-Centri Image-Segmentation-Evaluation.html#trimap-iou" class="sidebar-link">Trimap IoU</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[17]Boundary-IoU-Improving-Object-Centri Image-Segmentation-Evaluation.html#feature-measure" class="sidebar-link">Feature Measure</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[17]Boundary-IoU-Improving-Object-Centri Image-Segmentation-Evaluation.html#boundary-iou" class="sidebar-link">Boundary  IoU</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[17]Boundary-IoU-Improving-Object-Centri Image-Segmentation-Evaluation.html#尺度误差" class="sidebar-link">尺度误差</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[17]Boundary-IoU-Improving-Object-Centri Image-Segmentation-Evaluation.html#边界定位误差" class="sidebar-link">边界定位误差</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[17]Boundary-IoU-Improving-Object-Centri Image-Segmentation-Evaluation.html#物体定位误差" class="sidebar-link">物体定位误差</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[17]Boundary-IoU-Improving-Object-Centri Image-Segmentation-Evaluation.html#边界近似误差" class="sidebar-link">边界近似误差</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[17]Boundary-IoU-Improving-Object-Centri Image-Segmentation-Evaluation.html#内部掩码错误" class="sidebar-link">内部掩码错误</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[17]Boundary-IoU-Improving-Object-Centri Image-Segmentation-Evaluation.html#实现细节" class="sidebar-link">实现细节</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[17]Boundary-IoU-Improving-Object-Centri Image-Segmentation-Evaluation.html#mask-iou" class="sidebar-link">Mask IoU</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[17]Boundary-IoU-Improving-Object-Centri Image-Segmentation-Evaluation.html#理论分析" class="sidebar-link">理论分析</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[17]Boundary-IoU-Improving-Object-Centri Image-Segmentation-Evaluation.html#实证分析" class="sidebar-link">实证分析</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[17]Boundary-IoU-Improving-Object-Centri Image-Segmentation-Evaluation.html#总结" class="sidebar-link">总结</a></li></ul></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[17]Boundary-IoU-Improving-Object-Centri Image-Segmentation-Evaluation.html#trimap-iou-2" class="sidebar-link">Trimap IoU</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[17]Boundary-IoU-Improving-Object-Centri Image-Segmentation-Evaluation.html#f-measure" class="sidebar-link">F-measure</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[17]Boundary-IoU-Improving-Object-Centri Image-Segmentation-Evaluation.html#总结-2" class="sidebar-link">总结</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[17]Boundary-IoU-Improving-Object-Centri Image-Segmentation-Evaluation.html#公式" class="sidebar-link">公式</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[17]Boundary-IoU-Improving-Object-Centri Image-Segmentation-Evaluation.html#mask-iou-vs-boundary-iou-敏感性分析" class="sidebar-link">Mask IoU vs Boundary IoU：敏感性分析</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[17]Boundary-IoU-Improving-Object-Centri Image-Segmentation-Evaluation.html#boundary-iou-vs-trimap-iou" class="sidebar-link">Boundary IoU vs  Trimap IoU</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[17]Boundary-IoU-Improving-Object-Centri Image-Segmentation-Evaluation.html#boundary-iou-vs-f-measure" class="sidebar-link">Boundary IoU vs F-measure</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[17]Boundary-IoU-Improving-Object-Centri Image-Segmentation-Evaluation.html#像素距离参数d" class="sidebar-link">像素距离参数d</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[17]Boundary-IoU-Improving-Object-Centri Image-Segmentation-Evaluation.html#boundary-iou的局限" class="sidebar-link">Boundary IoU的局限</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[17]Boundary-IoU-Improving-Object-Centri Image-Segmentation-Evaluation.html#boundary-ap-for-instance-segmentation" class="sidebar-link">Boundary AP for instance segmentation</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[17]Boundary-IoU-Improving-Object-Centri Image-Segmentation-Evaluation.html#合成预测" class="sidebar-link">合成预测</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[17]Boundary-IoU-Improving-Object-Centri Image-Segmentation-Evaluation.html#真实预测" class="sidebar-link">真实预测</a></li></ul></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[17]Boundary-IoU-Improving-Object-Centri Image-Segmentation-Evaluation.html#boundary-pq" class="sidebar-link">Boundary  PQ</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[17]Boundary-IoU-Improving-Object-Centri Image-Segmentation-Evaluation.html#合成预测-2" class="sidebar-link">合成预测</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[17]Boundary-IoU-Improving-Object-Centri Image-Segmentation-Evaluation.html#真实预测-2" class="sidebar-link">真实预测</a></li></ul></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[17]Boundary-IoU-Improving-Object-Centri Image-Segmentation-Evaluation.html#和" class="sidebar-link">$Gd$和$Gd\cap G$</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[17]Boundary-IoU-Improving-Object-Centri Image-Segmentation-Evaluation.html#代码复现" class="sidebar-link">代码复现</a></li></ul></li><li><a href="/unlimited-paper-works/[18]Involution-Inverting-the-Inherence-of-Convolution-for-Visual-Recognition.html" class="sidebar-link">Involution: Inverting the Inherence of Convolution for Visual Recognition</a></li><li><a href="/unlimited-paper-works/[19]PointRend-Image-Segmentation-as-Rendering.html" class="sidebar-link">PointRend: Image Segmentation as Rendering</a></li><li><a href="/unlimited-paper-works/[20]Transformer-Attention-is-all-you-need.html" class="sidebar-link">Transformer: Attention is all you need</a></li><li><a href="/unlimited-paper-works/[21]RefineMask_Towards_High-Quality_Instance_Segmentationwith_Fine-Grained_Features.html" class="sidebar-link">RefineMask: Towards High-Quality Instance Segmentationwith Fine-Grained Features</a></li><li><a href="/unlimited-paper-works/[22]GLADNet-Low-Light-Enhancement-Network-with-Global-Awareness.html" class="sidebar-link">GLADNet: Low-Light Enhancement Network with Global Awareness</a></li><li><a href="/unlimited-paper-works/[23]Squeeze-and-Excitation-Networks.html" class="sidebar-link">Squeeze-and-Excitation Networks (SENet)</a></li><li><a href="/unlimited-paper-works/[24]BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation.html" class="sidebar-link">BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation</a></li><li><a href="/unlimited-paper-works/[25]Rethinking-BiSeNet-For-Real-time-Semantic-Segmentation.html" class="sidebar-link">Rethinking BiSeNet For Real-time Semantic Segmentation</a></li><li><a href="/unlimited-paper-works/[26]CBAM-Convolutional-Block-Attention-Module.html" class="sidebar-link">CBAM: Convolutional Block Attention Module</a></li><li><a href="/unlimited-paper-works/[27]Non-local-Neural-Networks.html" class="sidebar-link">Non-local Neural Networks</a></li><li><a href="/unlimited-paper-works/[28]GCNet-Non-local-Networks-Meet-Squeeze-Excitation-Networks-and-Beyond.html" class="sidebar-link">GCNet: Global Context Networks (Non-local Networks Meet Squeeze-Excitation Networks and Beyond)</a></li><li><a href="/unlimited-paper-works/[29]Disentangled-Non-Local-Neural-Networks.html" class="sidebar-link">Disentangled Non-Local Neural Networks</a></li><li><a href="/unlimited-paper-works/[30]RetinexNet-for-Low-Light-Enhancement.html" class="sidebar-link">Deep Retinex Decomposition for Low-Light Enhancement</a></li><li><a href="/unlimited-paper-works/[31]MSR-netLow-light-Image-Enhancement-Using-Deep-Convolutional-Network.html" class="sidebar-link">MSR-net:Low-light Image Enhancement Using Deep Convolutional Network</a></li><li><a href="/unlimited-paper-works/[32]LLCNN-A-Convolutional-Neural-Network-for-Low-light-Image-Enhancement.html" class="sidebar-link">LLCNN: A convolutional neural network for low-light image enhancement</a></li></ul></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>附录1：好朋友们</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>附录2：数学是真正的圣经</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>附录3：信号和采样的学问（DSP）</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>附录4：TensorFlow编程策略</span> <span class="arrow right"></span></p> <!----></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="boundary-iou-improving-object-centric-image-segmentation-evaluation"><a href="#boundary-iou-improving-object-centric-image-segmentation-evaluation" class="header-anchor">#</a> Boundary IoU: Improving Object-Centric Image Segmentation Evaluation</h1> <h3 id="这篇笔记的写作者是asthestarsfall。"><a href="#这篇笔记的写作者是asthestarsfall。" class="header-anchor">#</a> 这篇笔记的写作者是<a href="https://github.com/asthestarsfalll" target="_blank" rel="noopener noreferrer">AsTheStarsFall<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>。</h3> <blockquote><p>论文名称：<a href="https://arxiv.org/abs/2103.16562" target="_blank" rel="noopener noreferrer">Boundary IoU: Improving Object-Centric Image Segmentation Evaluation<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p>作者：Bowen Cheng，Ross Girshick，Piotr Dollár，Alexander C. Berg，Alexander Kirillov</p> <p>Code：https://github.com/bowenc0221/boundary-iou-api</p></blockquote> <p>写在前面：</p> <p>​	<strong>正如它的名字，Boundary IoU就是边界轮廓之间的IoU。</strong></p> <p>​	重点为3.4节、5.1节，其他基本都是对比实验。</p> <h1 id="摘要"><a href="#摘要" class="header-anchor">#</a> 摘要</h1> <ul><li>提出了一种新的基于边界质量的分割评价方法——Boundary IoU；</li> <li>Boundary IoU对大对象的边界误差比标准掩码IoU测量明显更敏感，并且不会过分惩罚较小对象的误差；</li> <li>比其他方法更适合作为评价分割的指标。</li></ul> <h1 id="介绍"><a href="#介绍" class="header-anchor">#</a> 介绍</h1> <ul><li><p>对于分割任务，不同的评估指标对不同类型错误的敏感性不同，网络可以轻易解决对应敏感的类型，而其他错误类型的效果则不尽人意；</p></li> <li><p>mask的边界质量是图像分割的一个重要指标，各种下游任务直接受益于更精确的目标分割；</p></li> <li><p>目前的分割网络的预测不够保真，边缘也很粗糙，<strong>这种情况说明目前的评估指标可能对目标边界的预测误差具有有限的敏感性</strong>；</p> <p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210508214210.png" alt="image-20210508214206239"></p></li> <li><p>在大量的论文中，AP最高可达到八九十，而很少有论文会提及他们mask的边界质量。</p></li> <li><p>对于实例分割，本文提出<strong>Boundary Average Precision</strong> (Boundary AP)，对于全景分割，提出<strong>Boundary Panop-tic Quality</strong> (Boundary PQ)。</p></li></ul> <h1 id="相关指标"><a href="#相关指标" class="header-anchor">#</a> 相关指标</h1> <p>各种相关指标如下：</p> <p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210508223158.png" alt="image-20210508222750295"></p> <p>首先解释几个名词：</p> <ol><li><p>对称（Symmetric）：GT（GroundTruth）和Pred（prediction）的交换是否改变测量值</p></li> <li><p>倾向（Preference）：衡量方法是否偏向某一类型的预测。</p></li> <li><p>不灵敏度（Insensitivity）：测量不太敏感的误差类型。</p></li> <li><p>三分图（Trimap）：对给定图像的一种粗略划分将给定图像划分为前景、背景和待求未知区域。</p> <img src="https://gitee.com/Thedeadleaf/images/raw/master/20210509151808.png" alt="img" style="zoom:150%;"></li> <li><p>Mask-based Measure：考虑物体的所有像素</p></li> <li><p>Boundary-based Measure：衡量预测边界的分割质量，不同于Mask-based Measure，该方法只评估边界及其邻近的像素。</p></li> <li><p>d：边界窄带的像素宽度</p></li></ol> <p>通过分析各种相关指标的缺点，我们得出Boundary IoU应该拥有的特性：<strong>同时考虑分类、定位和分割质量。</strong></p> <h2 id="mask-iou和pixel-accuracy"><a href="#mask-iou和pixel-accuracy" class="header-anchor">#</a> Mask IoU和Pixel Accuracy</h2> <p>所有像素对指标的贡献都是相同的，而物体内部的像素呈二次型增长，其边界仅会线性增长，因此<strong>对较大物体的边界不够敏感</strong>。</p> <p>Mask IoU计算方式示意图：</p> <img src="https://gitee.com/Thedeadleaf/images/raw/master/20210510161356.png" alt="image-20210510161350211" style="zoom:150%;"> <h2 id="trimap-iou"><a href="#trimap-iou" class="header-anchor">#</a> Trimap IoU</h2> <p>基于边界的分割指标，其计算距离GT和pred边界d像素窄带内的IoU，计算方式示意图如下（方便起见，简化为矩形且只显示边界部分）：</p> <img src="https://gitee.com/Thedeadleaf/images/raw/master/20210509163907.png" alt="image-20210509163853163" style="zoom:150%;"> <p><strong>需要注意分母的</strong><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>G</mi><mi>d</mi></msub><mo>∩</mo><mi>G</mi></mrow><annotation encoding="application/x-tex">G_d\cap G</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∩</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal">G</span></span></span></span>。</p> <h2 id="feature-measure"><a href="#feature-measure" class="header-anchor">#</a> Feature Measure</h2> <p>F-Measure最初被提出用于边缘检测，但它也被用于评价分割质量。在最初的公式中，使用二分图匹配来进行计算，对于高分辨率的图像来说计算成本很大；因此提出了一种允许重复匹配的近似算法，<strong>precision为pred轮廓中 \ 距离GT轮廓中像素 \ 在d个像素以内的 \ 像素 \ 所占pred的比例</strong>（已断句），recall同理。不是很理解，原文如下：</p> <p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210510151207.png" alt="image-20210510151147870"></p> <p>Precision和Recall计算方式示意图如下（可能）：</p> <img src="https://gitee.com/Thedeadleaf/images/raw/master/20210510153516.png" alt="image-20210510152547915" style="zoom:150%;"> <h2 id="boundary-iou"><a href="#boundary-iou" class="header-anchor">#</a> Boundary  IoU</h2> <p>Boundary IoU对大物体边界误差更加敏感，并且不会过分惩罚小物体。</p> <p>直观上就是GT和Pred轮廓的交集除以并集，但是<strong>这里的轮廓是在对象内部的</strong><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>G</mi><mi>d</mi></msub><mtext>、</mtext><msub><mi>P</mi><mi>d</mi></msub></mrow><annotation encoding="application/x-tex">G_d、P_d</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord cjk_fallback">、</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，不包括在对象外面的部分，详细请看9.1。</p> <p>虽然看起来和Trimap IoU很相似，但个人认为它是Mask IoU的边界升级版本，去除了对象内部巨量像素对整体的影响（见5.1Mask IoU的分析），使其拥有更优秀的性质。</p> <p>完整的论文中给出的示意图如下：</p> <p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210510154511.png" alt="image-20210510153535293"></p> <p>我画的：</p> <img src="https://gitee.com/Thedeadleaf/images/raw/master/20210510154514.png" alt="image-20210510154509338" style="zoom:150%;"> <h1 id="敏感性分析"><a href="#敏感性分析" class="header-anchor">#</a> 敏感性分析</h1> <p>为了进行系统的比较，本文对GT进行处理形成伪预测，通过<strong>模拟</strong>不同的误差类型来尽可能的模拟真实误差类型。</p> <h2 id="尺度误差"><a href="#尺度误差" class="header-anchor">#</a> 尺度误差</h2> <p>通过对GT进行膨胀和腐蚀操作，误差严重程度由运算核半径控制。</p> <p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210509185613.png" alt="image-20210509185608432"></p> <h2 id="边界定位误差"><a href="#边界定位误差" class="header-anchor">#</a> 边界定位误差</h2> <p>将随机高斯噪声添加到GT上每一个多边形顶点的<strong>坐标</strong>上，误差严重程度由高斯噪声的标准差确定。</p> <p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210509185617.png" alt="image-20210509185545908"></p> <h2 id="物体定位误差"><a href="#物体定位误差" class="header-anchor">#</a> 物体定位误差</h2> <p>将GT中的对象随机偏移一些像素，误差严重程度由位移像素长度控制。</p> <p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210509185622.png" alt="image-20210509185530435"></p> <h2 id="边界近似误差"><a href="#边界近似误差" class="header-anchor">#</a> 边界近似误差</h2> <p>利用Sharply的简化公式来删除多边形顶点，同时保持简化多边形尽可能接近原始图像，误差严重程度由函数的容错参数控制。</p> <p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210509185624.png" alt="image-20210509185108649"></p> <h2 id="内部掩码错误"><a href="#内部掩码错误" class="header-anchor">#</a> 内部掩码错误</h2> <p>向GT中添加随机性形状的孔，虽然这种误差类型并不常见，但是本文将其包含进来，用以评估内部掩膜误差的影响。</p> <p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210509185630.png" alt="image-20210509185508685"></p> <h2 id="实现细节"><a href="#实现细节" class="header-anchor">#</a> 实现细节</h2> <p><strong>数据集</strong>：作者从LVIS V0.5验证集中随机抽取实例掩码，因为该数据集拥有高质量的注释。</p> <p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210509190212.png" alt=""></p> <p><strong>实现过程</strong>：通过改变误差类型和误差的严重程度，记录每种类型的平均值和标准差，此外，还通过划分不同的区域，来比较对不同大小物体的指标评价。</p> <p>其中d设置为图像对角线的2%。</p> <h1 id="现有方法分析"><a href="#现有方法分析" class="header-anchor">#</a> 现有方法分析</h1> <h2 id="mask-iou"><a href="#mask-iou" class="header-anchor">#</a> Mask IoU</h2> <h3 id="理论分析"><a href="#理论分析" class="header-anchor">#</a> 理论分析</h3> <p><strong>尺度不变性</strong>（自己取的）：即对于一个<strong>固定</strong>的Mask IoU值，分割对象面积越大，则其错误像素越多，二者之间的变化关系成正比，其比例即为Mask IoU的值。</p> <p><strong>惩罚差异性</strong>（自己取的）：然而，当缩放一个对象时，内部像素数量呈二次增长，边界像素仅为线性增长，二者不同的增长率导致Mask IoU容忍更大的对象边界上的更多错误分类。</p> <h3 id="实证分析"><a href="#实证分析" class="header-anchor">#</a> 实证分析</h3> <p><strong>尺度不变性</strong>基于一个假设，即GT标注中的边界误差也随着对象的大小而增长。</p> <p>然而已有研究表明，不论物体大小，被不同标注器标记的同一个对象的两个轮廓之间的像素距离很少超过图像对角线的1%。（就叫它<strong>标注相似性</strong>吧）</p> <p>本文通过研究LVIS提供的双标注图像来证实这一点，如下：</p> <p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210509203230.png" alt="image-20210509201406961"></p> <p>其中冰箱的面积是机翼面积的100倍，但在相同分辨率的区域内，注释之间的差异在视觉上十分相似。</p> <p>两者的两个轮廓的Mask IoU分别为0.97,0.81，而它们的Boundary IoU则更为接近，分别为0.87，0.81。说明Mask IoU<strong>对小尺寸图片的“惩罚”更大</strong>。</p> <p><strong>实验</strong>：通过严重程度相同的膨胀/腐蚀来模拟<strong>尺度误差</strong>，其显著降低了小物体的Mask IoU，而Mask IoU随物体大小的增加而增加，见下图：</p> <p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210510093856.png" alt="image-20210510093853486"></p> <h3 id="总结"><a href="#总结" class="header-anchor">#</a> 总结</h3> <ul><li>Mask IoU的主要不足在于对大物体边界的不敏感性。</li> <li>相比之下，Boundary IoU更注重物体的边界。</li></ul> <h2 id="trimap-iou-2"><a href="#trimap-iou-2" class="header-anchor">#</a> Trimap IoU</h2> <p>Trimap IoU是不对称的，交换GT和Pred将会得到不同的值。下图显示了其更倾向于比GT更大的pred：</p> <p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210510095941.png" alt="image-20210510095821668"></p> <p>可以看到：</p> <ul><li>不论膨胀的严重程度是多少，其值总会大于某个正值，对小物体的“惩罚”依然过大。</li> <li>腐蚀则会下降到零。</li></ul> <p>简单的证明：</p> <p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210511114830.png" alt="image-20210510165235885"></p> <p>蓝色部分为pseudo-predictions （伪预测），红色方框为GT轮廓，可以看到，当pseudo-predictions 完全包含了GT时，其值不会再改变</p> <p>同理，当伪预测完全被GT所包含，分子为0，最终值为0。</p> <h2 id="f-measure"><a href="#f-measure" class="header-anchor">#</a> F-measure</h2> <p>F-measure完全忽略了小的轮廓误差，但是表现效果很差，会在很短的严重程度中快速下降到0：</p> <p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210511114828.png" alt="image-20210510170006064"></p> <h2 id="总结-2"><a href="#总结-2" class="header-anchor">#</a> 总结</h2> <p>综上可知，F-measure和Trimap IoU都不能代替Mask IoU，而Mask IoU也有着不能忽视的缺陷，因此，本文提出Boundary IoU。</p> <h1 id="boundary-iou-2"><a href="#boundary-iou-2" class="header-anchor">#</a> Boundary IoU</h1> <h2 id="公式"><a href="#公式" class="header-anchor">#</a> 公式</h2> <p>一个简化的IoU公式</p> <p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>I</mi><mi>o</mi><mi>U</mi><mo>=</mo><mfrac><mrow><msub><mi>G</mi><mi>d</mi></msub><mo>∩</mo><msub><mi>P</mi><mi>d</mi></msub></mrow><mrow><msub><mi>G</mi><mi>d</mi></msub><mo>∪</mo><msub><mi>P</mi><mi>d</mi></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">IoU = \frac{G_d\cap P_d}{G_d\cup P_d}
</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.10903em;">U</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.19633em;vertical-align:-0.8360000000000001em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.36033em;"><span style="top:-2.3139999999999996em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∪</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∩</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8360000000000001em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p> <p>该公式直接使用<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>G</mi><mi>d</mi></msub><mtext>、</mtext><msub><mi>P</mi><mi>d</mi></msub></mrow><annotation encoding="application/x-tex">G_d、P_d</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord cjk_fallback">、</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>,丢失了边缘的尖锐部分的信息</p> <p>Boundary IoU公式如下：</p> <p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>B</mi><mi>o</mi><mi>u</mi><mi>d</mi><mi>a</mi><mi>r</mi><mi>y</mi><mo>−</mo><mi>I</mi><mi>o</mi><mi>U</mi><mo stretchy="false">(</mo><mi>G</mi><mo separator="true">,</mo><mi>P</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi mathvariant="normal">∣</mi><mo stretchy="false">(</mo><msub><mi>G</mi><mi>d</mi></msub><mo>∩</mo><mi>G</mi><mo stretchy="false">)</mo><mo>∩</mo><mo stretchy="false">(</mo><msub><mi>P</mi><mi>d</mi></msub><mo>∩</mo><mi>P</mi><mo stretchy="false">)</mo><mi mathvariant="normal">∣</mi></mrow><mrow><mi mathvariant="normal">∣</mi><mo stretchy="false">(</mo><msub><mi>G</mi><mi>d</mi></msub><mo>∩</mo><mi>G</mi><mo stretchy="false">)</mo><mo>∪</mo><mo stretchy="false">(</mo><msub><mi>P</mi><mi>d</mi></msub><mo>∩</mo><mi>P</mi><mo stretchy="false">)</mo><mi mathvariant="normal">∣</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">Boudary-IoU(G,P)=\frac{|(G_d\cap G)\cap(P_d\cap P)|}{|(G_d\cap G)\cup(P_d\cap P)|}
</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mord mathnormal">o</span><span class="mord mathnormal">u</span><span class="mord mathnormal">d</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.03588em;">ry</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.10903em;">U</span><span class="mopen">(</span><span class="mord mathnormal">G</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.363em;vertical-align:-0.936em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">∣</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∩</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathnormal">G</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∪</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∩</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mclose">)</span><span class="mord">∣</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">∣</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∩</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathnormal">G</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∩</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∩</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mclose">)</span><span class="mord">∣</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p> <p>其中参数d控制了测量的灵敏性，当d足够大时，Boundary IoU就相当于Mask IoU;若使用较小的d，Boundary IoU则会忽略内部像素，使其对边界像素更加敏感。</p> <p>此外，对于较小的对象，Boundary IoU十分接近甚至等价于Mask IoU，这主要取决于参数d。</p> <h2 id="mask-iou-vs-boundary-iou-敏感性分析"><a href="#mask-iou-vs-boundary-iou-敏感性分析" class="header-anchor">#</a> Mask IoU vs Boundary IoU：敏感性分析</h2> <p>本文对比了Mask IoU和Boundary IoU在面积大于<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>9</mn><msup><mn>6</mn><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">96^2</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord">9</span><span class="mord"><span class="mord">6</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>的物体的不同误差类型下的表现：</p> <p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210510181123.png" alt="image-20210510173824215"></p> <p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210510181124.png" alt="image-20210510173839905"></p> <p>对于每种误差类型，Boundary IoU都能更好的利用0-1的范围</p> <p>使用的固定的误差严重程度，对大小不同的对象使用伪预测，以<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><msup><mn>6</mn><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">16^2</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord"><span class="mord">6</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>为增量划分区域，二者表现如下：</p> <p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210510181127.png" alt="image-20210510181102929"></p> <p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210510181129.png" alt="image-20210510181118302"></p> <p>可以看到：</p> <ul><li>对于较大的对象，Boundary IoU在相同严重程度下保持平缓，而Mask IoU则明显的偏向于大物体；</li> <li>对于较小的对象，二者拥有相似的指标，说明他们都没有对其进行过度惩罚。</li></ul> <h2 id="boundary-iou-vs-trimap-iou"><a href="#boundary-iou-vs-trimap-iou" class="header-anchor">#</a> Boundary IoU vs  Trimap IoU</h2> <p>二者具有一定的相似性，Boundary IoU将Pred和GT边缘上的像素都考虑了进来，这个简单的改进改变了Trimap IoU两点不足，一是不对称，二见4.2。</p> <h2 id="boundary-iou-vs-f-measure"><a href="#boundary-iou-vs-f-measure" class="header-anchor">#</a> Boundary IoU vs F-measure</h2> <p>F-measure对轮廓之间使用了硬预测——如果轮廓之间的像素在距离d内那么Precision和Recall都是完美的，然而当它们都位于d之外，则不会发生任何匹配（见4.3 ，其值会很快的降为0）。</p> <p>而Boundary IoU使用一种软分割，变化平缓。</p> <p>在附录中将会进行详细分析。</p> <h2 id="像素距离参数d"><a href="#像素距离参数d" class="header-anchor">#</a> 像素距离参数d</h2> <p>上文提过，当d足够大时，Boundary IoU等价于Mask IoU，当d过小，Boundary IoU则会出现严重惩罚的情况。</p> <p>为了选择合适的参数d，本文在COCO和ASE20K两个数据集（它们拥有相似的分辨率）上进行实验，发现当d为图像**对角线的2%（大约为15个像素）**时，两数据集的Boundary IoU的中位数超过0.9。</p> <p>对于Cityscapes中更大分辨率的图像，作者也建议使用相同的像素距离（15个左右），设置d为对角线的0.5%</p> <p>对于其他数据集，作者建议考虑两个因素（<strong>没看懂</strong>：</p> <ol><li>将注释一致性将下界设为d</li> <li>D应根据当前方法的性能选择，并随着性能的提高而降低。</li></ol> <h2 id="boundary-iou的局限"><a href="#boundary-iou的局限" class="header-anchor">#</a> Boundary IoU的局限</h2> <p>Boundary IoU不评估距离轮廓超过d的像素，例如一个圆形Mask和一个环形Mask：</p> <p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210511114822.png" alt="image-20210510190200706"></p> <p>显然，其Boundary Iou值极高为1</p> <p>为了惩罚这种情况，作者建议组合Boundary IoU和Mask IoU，并取他们的最小值。</p> <p>此外，在实验中还发现，99.9%的情况Boundary IoU都是小于等于Mask IoU的，极少数情况如上图会出现Boundary IoU大于Mask IoU。</p> <h1 id="应用"><a href="#应用" class="header-anchor">#</a> 应用</h1> <p>如上文所说，作者将两种IoU组合，取其最小。</p> <h2 id="boundary-ap-for-instance-segmentation"><a href="#boundary-ap-for-instance-segmentation" class="header-anchor">#</a> Boundary AP for instance segmentation</h2> <p>实例分割任务的目标是用像素级掩码描绘每个对象，其评估指标是同时评估多个方面，如分类、定位和分割质量。</p> <p>本文通过（Synthetic predictions，Synthetic，综合的；合成的，人造的，结合上下文个人感觉应该取“人造”之意） 合成预测与真实模型来进行实验。</p> <h3 id="合成预测"><a href="#合成预测" class="header-anchor">#</a> 合成预测</h3> <blockquote><p>综合预测允许我们单独的评估分割质量。</p></blockquote> <ul><li><p><strong>具体方法</strong>：</p> <p>使用COCO数据集，将GT缩小为28X28的连续值掩码，使用双线性插值upscale it back，最后将其二值化。如下图所示</p> <p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210510230303.png" alt="image-20210510230301360"></p> <p>这种合成Mask十分接近GT，但这种差异随着物体大小的增大而增大，因此越大的物体经过处理后的IoU值应该越低。</p> <p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210511122838.png" alt="image-20210510223226154"></p> <p>下标表示物体的大小，可以看到，对于越大的物体，Boundary IoU的值越低，而Mask IoU的值则维持在高水平，<strong>这进一步显示了Boundary IoU对于大物体边界的敏感性</strong>。</p></li> <li><p>实验结果：在Mask RCNN、PointRend、以及BMask RCNN模型上进行实验，结果如下：</p> <p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210511122836.png" alt="image-20210510224102719"></p> <p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210511122835.png" alt="image-20210510224120918"></p> <p>众所周知，Mask RCNN对大物体的分割表现不尽人意（我不知道），从上表可以看出Boundary Ap的优越性</p> <p>此外，上表还证明了相较于BMask RCNN，PointRend对较大对象的表现更好。</p> <p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210511122833.png" alt="image-20210510224713604"></p> <p>上表显示了更深的主干网络并不能带来分割质量的显著提升。</p></li></ul> <h3 id="真实预测"><a href="#真实预测" class="header-anchor">#</a> 真实预测</h3> <blockquote><p>利用现有的分割模型得到的真实预测进一步实验，可以进一步了解Boundary IoU在实例分割任务各个方面的表现。</p></blockquote> <ul><li><p><strong>具体方法</strong>：</p> <p>为了将分割质量与分类和定位错误分离开，作者为这些方法提供了Ground Truth Box，并为其分配随机置信度。</p></li> <li><p><strong>实验结果</strong>：</p> <p>模型在COCO数据集上训练，在LVIS v0.5上验证</p> <p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210511115505.png" alt="image-20210511115502161"></p> <p>模型在Cityscapes上训练和验证</p> <p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210511115657.png" alt="image-20210511115655795"></p></li></ul> <h2 id="boundary-pq"><a href="#boundary-pq" class="header-anchor">#</a> Boundary  PQ</h2> <p>下图为标准PQ的公式</p> <img src="https://gitee.com/Thedeadleaf/images/raw/master/20210511115040.png" alt="image-20210511115032369" style="zoom:150%;"> <p>将其中的Mask IoU替换为Mask IoU与Boundary IoU的组合，取其最小值。</p> <h3 id="合成预测-2"><a href="#合成预测-2" class="header-anchor">#</a> 合成预测</h3> <img src="https://gitee.com/Thedeadleaf/images/raw/master/20210511122826.png" alt="image-20210511120047274" style="zoom:150%;"> <h3 id="真实预测-2"><a href="#真实预测-2" class="header-anchor">#</a> 真实预测</h3> <p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210511122824.png" alt="image-20210511120131765"></p> <h1 id="总结-3"><a href="#总结-3" class="header-anchor">#</a> 总结</h1> <p>​		不同于Mask IoU，Boundary IoU提供了一个明确的，定量的梯度，奖励改善边界分割质量。作者希望Boundary IoU可以鼓励更多人开发高保真Mask预测新方法。此外，Boundary  IoU允许对复杂的任务(如实例和全景分割)的分割相关错误进行更细粒度的分析。在性能分析工具(如TIDE[2])中结合度量可以更好地洞察实例分段模型的特定错误类型。（<strong>直接翻译的</strong>）</p> <h1 id="补充"><a href="#补充" class="header-anchor">#</a> 补充</h1> <h2 id="和"><a href="#和" class="header-anchor">#</a> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>G</mi><mi>d</mi></msub></mrow><annotation encoding="application/x-tex">G_d</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>G</mi><mi>d</mi></msub><mo>∩</mo><mi>G</mi></mrow><annotation encoding="application/x-tex">G_d\cap G</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∩</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal">G</span></span></span></span></h2> <p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210511121300.png" alt="image-20210511121230297"></p> <h2 id="代码复现"><a href="#代码复现" class="header-anchor">#</a> 代码复现</h2> <p>对于二分类图像的Boundary Iou</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token comment"># 将二值Mask转化为Boundary mask</span>
<span class="token keyword">def</span> <span class="token function">mask_to_boundary</span><span class="token punctuation">(</span>mask<span class="token punctuation">,</span> dilation_ratio<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    h<span class="token punctuation">,</span> w <span class="token operator">=</span> mask<span class="token punctuation">.</span>shape
    img_diag <span class="token operator">=</span> np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>h <span class="token operator">**</span> <span class="token number">2</span> <span class="token operator">+</span> w <span class="token operator">**</span> <span class="token number">2</span><span class="token punctuation">)</span>
    dilation <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span><span class="token builtin">round</span><span class="token punctuation">(</span>dilation_ratio <span class="token operator">*</span> img_diag<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> dilation <span class="token operator">&lt;</span> <span class="token number">1</span><span class="token punctuation">:</span>
        dilation <span class="token operator">=</span> <span class="token number">1</span>
    <span class="token comment"># Pad image so mask truncated by the image border is also considered as boundary.</span>
    <span class="token comment"># 将mask使用0填充一圈，防止dilation为1时</span>
    new_mask <span class="token operator">=</span> cv2<span class="token punctuation">.</span>copyMakeBorder<span class="token punctuation">(</span>
        mask<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> cv2<span class="token punctuation">.</span>BORDER_CONSTANT<span class="token punctuation">,</span> value<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
    kernel <span class="token operator">=</span> np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>uint8<span class="token punctuation">)</span>
    <span class="token comment"># 对mask进行腐蚀操作</span>
    new_mask_erode <span class="token operator">=</span> cv2<span class="token punctuation">.</span>erode<span class="token punctuation">(</span>new_mask<span class="token punctuation">,</span> kernel<span class="token punctuation">,</span> iterations<span class="token operator">=</span>dilation<span class="token punctuation">)</span>
    mask_erode <span class="token operator">=</span> new_mask_erode<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span> h <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span> w <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span>
    <span class="token comment"># G_d intersects G</span>
    <span class="token keyword">return</span> mask <span class="token operator">-</span> mask_erode

<span class="token keyword">def</span> <span class="token function">boundary_iou</span><span class="token punctuation">(</span>mask<span class="token punctuation">,</span> pred<span class="token punctuation">)</span><span class="token punctuation">:</span>
    intersect <span class="token operator">=</span> mask<span class="token operator">*</span>pred
    ite <span class="token operator">=</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>intersect <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">)</span>
    un <span class="token operator">=</span> mask<span class="token operator">+</span>pred
    union <span class="token operator">=</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>un <span class="token operator">&gt;=</span> <span class="token number">1</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> ite<span class="token operator">/</span>union
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br></div></div><p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210519091830.png" alt="image-20210519091807762"></p> <p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210519091840.png" alt="image-20210519091815466"></p> <p><img src="https://gitee.com/Thedeadleaf/images/raw/master/20210519091833.png" alt="image-20210519091826066"></p></div> <footer class="page-edit"><!----> <div class="last-updated"><span class="prefix">上次更新时间:</span> <span class="time">2021/7/27 上午4:27:40</span></div></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/unlimited-paper-works/[16]Convolutional-Block-Attention-Module.html" class="prev">
        Convolutional Block Attention Module
      </a></span> <span class="next"><a href="/unlimited-paper-works/[18]Involution-Inverting-the-Inherence-of-Convolution-for-Visual-Recognition.html">
        Involution: Inverting the Inherence of Convolution for Visual Recognition
      </a>
      →
    </span></p></div> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.0cf0d483.js" defer></script><script src="/assets/js/2.d537feee.js" defer></script><script src="/assets/js/98.ca3ecf81.js" defer></script>
  </body>
</html>
