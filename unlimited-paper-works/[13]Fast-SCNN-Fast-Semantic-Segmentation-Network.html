<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Fast-SCNN: Fast Semantic Segmentation Network | 工具箱的深度学习记事簿</title>
    <meta name="generator" content="VuePress 1.8.2">
    <link rel="icon" href="/statics/logo.svg">
    <meta name="description" content="这里包含了我从入门到依然在入门的过程中接触到的大部分知识。翻翻目录，也许能找到有用的">
    <meta name="keywords" content="Akasaki,Deep learning,Machine learning,工具箱,工具箱的深度学习记事簿,Akasaki的深度学习记事簿">
    <meta name="google-site-verification" content="VVNYs0bXM_EKTgxJ8XIfvXShjHsksGNv3YNedxBGFjU">
    
    <link rel="preload" href="/assets/css/0.styles.b2187a0e.css" as="style"><link rel="preload" href="/assets/js/app.0cf0d483.js" as="script"><link rel="preload" href="/assets/js/2.d537feee.js" as="script"><link rel="preload" href="/assets/js/28.35498a04.js" as="script"><link rel="prefetch" href="/assets/js/10.000b9996.js"><link rel="prefetch" href="/assets/js/100.01b72e43.js"><link rel="prefetch" href="/assets/js/101.ab99304f.js"><link rel="prefetch" href="/assets/js/102.b1ad1ca3.js"><link rel="prefetch" href="/assets/js/11.0ec32f4a.js"><link rel="prefetch" href="/assets/js/12.7ad41a4b.js"><link rel="prefetch" href="/assets/js/13.40c87056.js"><link rel="prefetch" href="/assets/js/14.2d8bcb8d.js"><link rel="prefetch" href="/assets/js/15.a7295b50.js"><link rel="prefetch" href="/assets/js/16.7158147a.js"><link rel="prefetch" href="/assets/js/17.9215c05c.js"><link rel="prefetch" href="/assets/js/18.582ac004.js"><link rel="prefetch" href="/assets/js/19.7cc8fc31.js"><link rel="prefetch" href="/assets/js/20.1b1e2af4.js"><link rel="prefetch" href="/assets/js/21.b2dadf61.js"><link rel="prefetch" href="/assets/js/22.b19748fc.js"><link rel="prefetch" href="/assets/js/23.f2db4fa1.js"><link rel="prefetch" href="/assets/js/24.3c31aece.js"><link rel="prefetch" href="/assets/js/25.531fd9fb.js"><link rel="prefetch" href="/assets/js/26.6e841782.js"><link rel="prefetch" href="/assets/js/27.33add335.js"><link rel="prefetch" href="/assets/js/29.aabed81c.js"><link rel="prefetch" href="/assets/js/3.0a121536.js"><link rel="prefetch" href="/assets/js/30.54f44e16.js"><link rel="prefetch" href="/assets/js/31.8ecbe721.js"><link rel="prefetch" href="/assets/js/32.873f263a.js"><link rel="prefetch" href="/assets/js/33.106c15c8.js"><link rel="prefetch" href="/assets/js/34.349d6fdb.js"><link rel="prefetch" href="/assets/js/35.73569c69.js"><link rel="prefetch" href="/assets/js/36.24f08107.js"><link rel="prefetch" href="/assets/js/37.da460b69.js"><link rel="prefetch" href="/assets/js/38.2f6c4646.js"><link rel="prefetch" href="/assets/js/39.435f8032.js"><link rel="prefetch" href="/assets/js/4.972cd905.js"><link rel="prefetch" href="/assets/js/40.4760c47b.js"><link rel="prefetch" href="/assets/js/41.a7cc5e5d.js"><link rel="prefetch" href="/assets/js/42.fc3e47f2.js"><link rel="prefetch" href="/assets/js/43.a13c67d4.js"><link rel="prefetch" href="/assets/js/44.46c317cb.js"><link rel="prefetch" href="/assets/js/45.f11cdfc6.js"><link rel="prefetch" href="/assets/js/46.541bd27d.js"><link rel="prefetch" href="/assets/js/47.5705cba6.js"><link rel="prefetch" href="/assets/js/48.d2040c65.js"><link rel="prefetch" href="/assets/js/49.cbb5c42b.js"><link rel="prefetch" href="/assets/js/5.e2cb1c20.js"><link rel="prefetch" href="/assets/js/50.39732f79.js"><link rel="prefetch" href="/assets/js/51.99b82adb.js"><link rel="prefetch" href="/assets/js/52.808c9ece.js"><link rel="prefetch" href="/assets/js/53.67e417da.js"><link rel="prefetch" href="/assets/js/54.1c722ed6.js"><link rel="prefetch" href="/assets/js/55.afc1aceb.js"><link rel="prefetch" href="/assets/js/56.a463c15a.js"><link rel="prefetch" href="/assets/js/57.f90cb51c.js"><link rel="prefetch" href="/assets/js/58.8e11cf82.js"><link rel="prefetch" href="/assets/js/59.ed4fc789.js"><link rel="prefetch" href="/assets/js/6.59e5684c.js"><link rel="prefetch" href="/assets/js/60.adcefa1d.js"><link rel="prefetch" href="/assets/js/61.a6446644.js"><link rel="prefetch" href="/assets/js/62.1721a24b.js"><link rel="prefetch" href="/assets/js/63.726a6135.js"><link rel="prefetch" href="/assets/js/64.e2db755f.js"><link rel="prefetch" href="/assets/js/65.e92b9d46.js"><link rel="prefetch" href="/assets/js/66.b4d3e87e.js"><link rel="prefetch" href="/assets/js/67.5f3eaf29.js"><link rel="prefetch" href="/assets/js/68.85b4615c.js"><link rel="prefetch" href="/assets/js/69.63d409f1.js"><link rel="prefetch" href="/assets/js/7.b3ddb4c6.js"><link rel="prefetch" href="/assets/js/70.6f3f7332.js"><link rel="prefetch" href="/assets/js/71.0a087c8a.js"><link rel="prefetch" href="/assets/js/72.6513570f.js"><link rel="prefetch" href="/assets/js/73.896f4d9c.js"><link rel="prefetch" href="/assets/js/74.ecc3d286.js"><link rel="prefetch" href="/assets/js/75.b06a8964.js"><link rel="prefetch" href="/assets/js/76.3032add1.js"><link rel="prefetch" href="/assets/js/77.4fb4de51.js"><link rel="prefetch" href="/assets/js/78.42cf5412.js"><link rel="prefetch" href="/assets/js/79.0ba88830.js"><link rel="prefetch" href="/assets/js/8.bb97c23d.js"><link rel="prefetch" href="/assets/js/80.aaa6656c.js"><link rel="prefetch" href="/assets/js/81.4a1804f1.js"><link rel="prefetch" href="/assets/js/82.7ad53f0e.js"><link rel="prefetch" href="/assets/js/83.c4173111.js"><link rel="prefetch" href="/assets/js/84.cb7300ea.js"><link rel="prefetch" href="/assets/js/85.81c0ea84.js"><link rel="prefetch" href="/assets/js/86.317fcc1c.js"><link rel="prefetch" href="/assets/js/87.6f50a42f.js"><link rel="prefetch" href="/assets/js/88.898a4cd8.js"><link rel="prefetch" href="/assets/js/89.ddbe5c5c.js"><link rel="prefetch" href="/assets/js/9.596e7f45.js"><link rel="prefetch" href="/assets/js/90.c1e40160.js"><link rel="prefetch" href="/assets/js/91.f85fdec8.js"><link rel="prefetch" href="/assets/js/92.1d7822a2.js"><link rel="prefetch" href="/assets/js/93.4ee2ef21.js"><link rel="prefetch" href="/assets/js/94.3760a5b4.js"><link rel="prefetch" href="/assets/js/95.b5be500a.js"><link rel="prefetch" href="/assets/js/96.e570f48e.js"><link rel="prefetch" href="/assets/js/97.af175cfb.js"><link rel="prefetch" href="/assets/js/98.ca3ecf81.js"><link rel="prefetch" href="/assets/js/99.2fc27711.js">
    <link rel="stylesheet" href="/assets/css/0.styles.b2187a0e.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><!----> <span class="site-name">工具箱的深度学习记事簿</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="https://github.com/VisualDust/ml.akasaki.space" target="_blank" rel="noopener noreferrer" class="nav-link external">
  View on Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div><div class="nav-item"><a href="https://github.com/VisualDust" target="_blank" rel="noopener noreferrer" class="nav-link external">
  工具箱
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="https://github.com/VisualDust/ml.akasaki.space" target="_blank" rel="noopener noreferrer" class="nav-link external">
  View on Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div><div class="nav-item"><a href="https://github.com/VisualDust" target="_blank" rel="noopener noreferrer" class="nav-link external">
  工具箱
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第零章：在开始之前</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第一章上：HelloWorld</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第一章下：深度学习基础</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第二章上：卷积神经网络</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第二章下：经典卷积神经网络</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第三章上：谈一些计算机视觉方向</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第三章下：了解更高级的技术</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>魔法部日志（又名论文阅读日志）</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/unlimited-paper-works/[0]unlimited-paper-works.html" class="sidebar-link">欢迎来到魔法部日志</a></li><li><a href="/unlimited-paper-works/[1]The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs.html" class="sidebar-link">The Devil is in the Decoder: Classification, Regression and GANs</a></li><li><a href="/unlimited-paper-works/[2]Threat-of-Adversarial-Attacks-on-Deep-Learning-in-Computer-Vision-A-Survey.html" class="sidebar-link">Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey</a></li><li><a href="/unlimited-paper-works/[3]Progressive-Semantic-Segmentation.html" class="sidebar-link">Progressive Semantic Segmentation</a></li><li><a href="/unlimited-paper-works/[4]Decoders-Matter-for-Semantic-Segmentation-Data-Dependent-Decoding-Enables-Flexible-Feature-Aggregation.html" class="sidebar-link">Decoders Matter for Semantic Segmentation: Data-Dependent Decoding Enables Flexible Feature Aggregation</a></li><li><a href="/unlimited-paper-works/[5]HLA-Face-Joint-High-Low-Adaptation-for-Low-Light-Face-Detection.html" class="sidebar-link">HLA-Face Joint High-Low Adaptation for Low Light Face Detection</a></li><li><a href="/unlimited-paper-works/[6]DeepLab-Series.html" class="sidebar-link">DeepLab Series</a></li><li><a href="/unlimited-paper-works/[7]Cross-Dataset-Collaborative-Learning-for-Semantic-Segmentation.html" class="sidebar-link">Cross-Dataset Collaborative Learning for Semantic Segmentation</a></li><li><a href="/unlimited-paper-works/[8]Dynamic-Neural-Networks-A-Survey.html" class="sidebar-link">Dynamic Neural Networks: A Survey</a></li><li><a href="/unlimited-paper-works/[9]Feature-Pyramid-Networks-for-Object-Detection.html" class="sidebar-link">Feature Pyramid Networks for Object Detection</a></li><li><a href="/unlimited-paper-works/[10]Overview-Of-Semantic-Segmentation.html" class="sidebar-link">A Review on Deep Learning Techniques Applied to Semantic Segmentation</a></li><li><a href="/unlimited-paper-works/[11]Image-Segmentation-Using-Deep-Learning-A-Survey.html" class="sidebar-link">Image Segmentation Using Deep Learning: A Survey</a></li><li><a href="/unlimited-paper-works/[12]MobileNetV2-Inverted-Residuals-and-Linear-bottleneck.html" class="sidebar-link">MobileNetV2: Inverted Residuals and Linear Bottlenecks</a></li><li><a href="/unlimited-paper-works/[13]Fast-SCNN-Fast-Semantic-Segmentation-Network.html" class="active sidebar-link">Fast-SCNN: Fast Semantic Segmentation Network</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[13]Fast-SCNN-Fast-Semantic-Segmentation-Network.html#摘要" class="sidebar-link">摘要</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[13]Fast-SCNN-Fast-Semantic-Segmentation-Network.html#dcnns的效率" class="sidebar-link">DCNNs的效率</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[13]Fast-SCNN-Fast-Semantic-Segmentation-Network.html#深度可分离卷积-depthwise-separable-convolutions" class="sidebar-link">深度可分离卷积(Depthwise Separable Convolutions):</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[13]Fast-SCNN-Fast-Semantic-Segmentation-Network.html#dcnns的高效重新设计" class="sidebar-link">DCNNs的高效重新设计</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[13]Fast-SCNN-Fast-Semantic-Segmentation-Network.html#网络压缩" class="sidebar-link">网络压缩</a></li></ul></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[13]Fast-SCNN-Fast-Semantic-Segmentation-Network.html#fast-scnn" class="sidebar-link">Fast-SCNN</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[13]Fast-SCNN-Fast-Semantic-Segmentation-Network.html#下采样学习模块-learning-to-down-sample" class="sidebar-link">下采样学习模块(learning to down-sample)</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[13]Fast-SCNN-Fast-Semantic-Segmentation-Network.html#全局特征提取器-global-feature-extrator" class="sidebar-link">全局特征提取器(Global Feature Extrator)</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[13]Fast-SCNN-Fast-Semantic-Segmentation-Network.html#特征融合模块-feature-fusion-module" class="sidebar-link">特征融合模块(Feature Fusion Module)</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[13]Fast-SCNN-Fast-Semantic-Segmentation-Network.html#分类模块-classifier" class="sidebar-link">分类模块(classifier)</a></li></ul></li></ul></li><li><a href="/unlimited-paper-works/[14]MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications.html" class="sidebar-link">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</a></li><li><a href="/unlimited-paper-works/[15]Gated-Channel-Transformation-for-Visual-Recognition.html" class="sidebar-link">Gated Channel Transformation for Visual Recognition</a></li><li><a href="/unlimited-paper-works/[16]Convolutional-Block-Attention-Module.html" class="sidebar-link">Convolutional Block Attention Module</a></li><li><a href="/unlimited-paper-works/[17]Boundary-IoU-Improving-Object-Centri Image-Segmentation-Evaluation.html" class="sidebar-link">Boundary IoU: Improving Object-Centric Image Segmentation Evaluation</a></li><li><a href="/unlimited-paper-works/[18]Involution-Inverting-the-Inherence-of-Convolution-for-Visual-Recognition.html" class="sidebar-link">Involution: Inverting the Inherence of Convolution for Visual Recognition</a></li><li><a href="/unlimited-paper-works/[19]PointRend-Image-Segmentation-as-Rendering.html" class="sidebar-link">PointRend: Image Segmentation as Rendering</a></li><li><a href="/unlimited-paper-works/[20]Transformer-Attention-is-all-you-need.html" class="sidebar-link">Transformer: Attention is all you need</a></li><li><a href="/unlimited-paper-works/[21]RefineMask_Towards_High-Quality_Instance_Segmentationwith_Fine-Grained_Features.html" class="sidebar-link">RefineMask: Towards High-Quality Instance Segmentationwith Fine-Grained Features</a></li><li><a href="/unlimited-paper-works/[22]GLADNet-Low-Light-Enhancement-Network-with-Global-Awareness.html" class="sidebar-link">GLADNet: Low-Light Enhancement Network with Global Awareness</a></li><li><a href="/unlimited-paper-works/[23]Squeeze-and-Excitation-Networks.html" class="sidebar-link">Squeeze-and-Excitation Networks (SENet)</a></li><li><a href="/unlimited-paper-works/[24]BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation.html" class="sidebar-link">BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation</a></li><li><a href="/unlimited-paper-works/[25]Rethinking-BiSeNet-For-Real-time-Semantic-Segmentation.html" class="sidebar-link">Rethinking BiSeNet For Real-time Semantic Segmentation</a></li><li><a href="/unlimited-paper-works/[26]CBAM-Convolutional-Block-Attention-Module.html" class="sidebar-link">CBAM: Convolutional Block Attention Module</a></li><li><a href="/unlimited-paper-works/[27]Non-local-Neural-Networks.html" class="sidebar-link">Non-local Neural Networks</a></li><li><a href="/unlimited-paper-works/[28]GCNet-Non-local-Networks-Meet-Squeeze-Excitation-Networks-and-Beyond.html" class="sidebar-link">GCNet: Global Context Networks (Non-local Networks Meet Squeeze-Excitation Networks and Beyond)</a></li><li><a href="/unlimited-paper-works/[29]Disentangled-Non-Local-Neural-Networks.html" class="sidebar-link">Disentangled Non-Local Neural Networks</a></li><li><a href="/unlimited-paper-works/[30]RetinexNet-for-Low-Light-Enhancement.html" class="sidebar-link">Deep Retinex Decomposition for Low-Light Enhancement</a></li><li><a href="/unlimited-paper-works/[31]MSR-netLow-light-Image-Enhancement-Using-Deep-Convolutional-Network.html" class="sidebar-link">MSR-net:Low-light Image Enhancement Using Deep Convolutional Network</a></li><li><a href="/unlimited-paper-works/[32]LLCNN-A-Convolutional-Neural-Network-for-Low-light-Image-Enhancement.html" class="sidebar-link">LLCNN: A convolutional neural network for low-light image enhancement</a></li></ul></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>附录1：好朋友们</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>附录2：数学是真正的圣经</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>附录3：信号和采样的学问（DSP）</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>附录4：TensorFlow编程策略</span> <span class="arrow right"></span></p> <!----></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="fast-scnn-fast-semantic-segmentation-network"><a href="#fast-scnn-fast-semantic-segmentation-network" class="header-anchor">#</a> Fast-SCNN: Fast Semantic Segmentation Network</h1> <h3 id="这篇笔记的写作者是zerorains。"><a href="#这篇笔记的写作者是zerorains。" class="header-anchor">#</a> 这篇笔记的写作者是<a href="https://github.com/zeroRains" target="_blank" rel="noopener noreferrer">ZeroRains<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>。</h3> <p>这是一篇讲解一种快速语义分割的论文。论文名:<a href="https://arxiv.org/abs/1902.04502" target="_blank" rel="noopener noreferrer">Fast-SCNN: Fast Semantic Segmentation Network<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <ul><li>主要是采用双流模型的架构设计这个网络</li> <li>本文总思路：减少冗余的卷积过程，从而提高速度</li></ul> <p>摘要：</p> <blockquote><p>The encoder-decoder framework is state-of-the-art for offline semantic image segmentation. Since the rise in autonomous systems, real-time computation is increasingly desirable. In this paper, we introduce fast segmentation convolutional neural network (Fast-SCNN), an above real-time semantic segmentation model on high resolution image data (1024 × 2048px) suited to efficient computation on embedded devices with low memory. Building on existing two-branch methods for fast segmentation, we introduce our ‘learning to downsample’ module which computes low-level features for multiple resolution branches simultaneously. Our network combines spatial detail at high resolution with deep features extracted at lower resolution, yielding an accuracy of 68.0% mean intersection over union at 123.5 frames per second on Cityscapes. We also show that large scale pre-training is unnecessary. We thoroughly validate our metric in experiments with ImageNet pre-training and the coarse labeled data of Cityscapes. Finally, we show even faster computation with competitive results on subsampled inputs, without any network modifications.</p></blockquote> <h2 id="摘要"><a href="#摘要" class="header-anchor">#</a> 摘要</h2> <p>主要贡献：</p> <ol><li>提出了一个有竞争性(68.0%miou)，并且能在高分辨率(1024x2048)的图片实现实时(123.5FPS)语义分割的算法Fast-SCNN.</li> <li>采用了离线型DCNNs中流行的<strong>跳跃连接(skip connection)</strong>，并提出了一种浅层学习的下采样模块<strong>learning to Down-sample</strong>,以此更加快速高效地进行多分支低级特征提取。</li> <li>将Fast-SCNN设计为轻量型(low capacity)，并证实了无论是使用ImageNet数据集的训练模型多训练几代，还是在添加的粗糙数据中多训练几代的结果是等效的。</li></ol> <h2 id="dcnns的效率"><a href="#dcnns的效率" class="header-anchor">#</a> DCNNs的效率</h2> <p>高效DCNNs（Diffusion-Convolutional Neural Networks ）的常见技术为：</p> <h3 id="深度可分离卷积-depthwise-separable-convolutions"><a href="#深度可分离卷积-depthwise-separable-convolutions" class="header-anchor">#</a> 深度可分离卷积(Depthwise Separable Convolutions):</h3> <p>MoblieNet将标准的Conv分解为**深度卷积（depthwise convolutions）**和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">1 \times 1</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>**点式卷积(pointwise convolution)**通过这样的方式，减少了浮点运算和卷积参数，减少了模型的计算成本和内存需求。</p> <h3 id="dcnns的高效重新设计"><a href="#dcnns的高效重新设计" class="header-anchor">#</a> DCNNs的高效重新设计</h3> <p>MobileNet-V2使用**倒置的瓶颈残差块(inverted bottleneck residual blocks)**以分类任务构建有效的DCNN。</p> <p>ContextNeto使用能够倒置瓶颈残差块设计了一个1两分支网络，以进行有效的实时语义分割。</p> <h3 id="网络压缩"><a href="#网络压缩" class="header-anchor">#</a> 网络压缩</h3> <p>使用剪枝减小预训练网络的大小，从而实现更块的运行时间，更小的参数集和更小的内存占用空间。</p> <p>Fast-SCNN严重依赖与深度可分离卷积和残差瓶颈块，还引入了一个两分支模型，该模型将学习内容整合到下采样的模块中，从而允许在多个分辨率级别上进行共享特征提取。网络量化和网络压缩可以正交应用，留待后面的工作。</p> <h2 id="fast-scnn"><a href="#fast-scnn" class="header-anchor">#</a> Fast-SCNN</h2> <p>网络结构图：</p> <p><img src="/assets/img/20210512115432image-20210512115431363.8d39d92d.png" alt="image-20210512115431363"></p> <p>在定义网络的BN层时使用类各种类型的BN层,但是默认都是使用普通的BN层</p> <p>常规的BN，SyncBN（跨卡BN），FrozenBN（测试阶段使用的BN），GN（Group Normalization）</p> <p><img src="/assets/img/20210516102348image-20210516102346560.90912384.png" alt="image-20210516102346560"></p> <h3 id="下采样学习模块-learning-to-down-sample"><a href="#下采样学习模块-learning-to-down-sample" class="header-anchor">#</a> 下采样学习模块(learning to down-sample)</h3> <p>在该模块中使用了三层卷积，第一层是普通的卷积(Conv2D)，其余两层是可分离卷积(DSConv)，因为图像刚刚输入只有三个通道，使用DSConv的优势并不明显所以，采用普通卷积层。</p> <p>在下采样学习模块中，使用的步长均为2，然后进行BN和ReLU。卷积核和深度可分离卷积核均为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mo>×</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">3\times3</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">3</span></span></span></span>.</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token comment"># 在主网络中的定义</span>
self<span class="token punctuation">.</span>learning_to_downsample <span class="token operator">=</span> LearningToDownsample<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">48</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> norm_layer<span class="token operator">=</span>self<span class="token punctuation">.</span>norm_layer<span class="token punctuation">)</span> <span class="token comment"># norm_layerh是普通的BN</span>

<span class="token comment"># 下采样学习模块的定义</span>
<span class="token keyword">class</span> <span class="token class-name">LearningToDownsample</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;Learning to downsample module&quot;&quot;&quot;</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dw_channels1<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> dw_channels2<span class="token operator">=</span><span class="token number">48</span><span class="token punctuation">,</span> out_channels<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> norm_layer<span class="token operator">=</span>nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>LearningToDownsample<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv <span class="token operator">=</span> _ConvBNReLU<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> dw_channels1<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment"># 这个就是单纯的CONV+BN+ReLU</span>
        <span class="token comment"># 深度可分离卷积：一个深度卷积，一个点卷积的组合</span>
        self<span class="token punctuation">.</span>dsconv1 <span class="token operator">=</span> SeparableConv2d<span class="token punctuation">(</span>dw_channels1<span class="token punctuation">,</span> dw_channels2<span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> relu_first<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> norm_layer<span class="token operator">=</span>norm_layer<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dsconv2 <span class="token operator">=</span> SeparableConv2d<span class="token punctuation">(</span>dw_channels2<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> relu_first<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> norm_layer<span class="token operator">=</span>norm_layer<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token comment">#  普通卷积</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>dsconv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token comment"># 可分离卷积</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>dsconv2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br></div></div><div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token comment"># 深度可分离卷积</span>
<span class="token keyword">class</span> <span class="token class-name">SeparableConv2d</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inplanes<span class="token punctuation">,</span> planes<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> dilation<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> relu_first<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                 bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> norm_layer<span class="token operator">=</span>nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 深度卷积，卷积核为3步长1，padding1，空洞1的卷积层</span>
        depthwise <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>inplanes<span class="token punctuation">,</span> inplanes<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span>
                              stride<span class="token operator">=</span>stride<span class="token punctuation">,</span> padding<span class="token operator">=</span>dilation<span class="token punctuation">,</span>
                              dilation<span class="token operator">=</span>dilation<span class="token punctuation">,</span> groups<span class="token operator">=</span>inplanes<span class="token punctuation">,</span> bias<span class="token operator">=</span>bias<span class="token punctuation">)</span>
        <span class="token comment"># 对应的BN</span>
        bn_depth <span class="token operator">=</span> norm_layer<span class="token punctuation">(</span>inplanes<span class="token punctuation">)</span>
        <span class="token comment"># 点卷积，就是普通的1x1卷积</span>
        pointwise <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>inplanes<span class="token punctuation">,</span> planes<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span>bias<span class="token punctuation">)</span>
        <span class="token comment"># 对应的BN</span>
        bn_point <span class="token operator">=</span> norm_layer<span class="token punctuation">(</span>planes<span class="token punctuation">)</span>
        <span class="token comment"># 是否使用激活函数</span>
        <span class="token keyword">if</span> relu_first<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>block <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>OrderedDict<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'relu'</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                                    <span class="token punctuation">(</span><span class="token string">'depthwise'</span><span class="token punctuation">,</span> depthwise<span class="token punctuation">)</span><span class="token punctuation">,</span>
                                                    <span class="token punctuation">(</span><span class="token string">'bn_depth'</span><span class="token punctuation">,</span> bn_depth<span class="token punctuation">)</span><span class="token punctuation">,</span>
                                                    <span class="token punctuation">(</span><span class="token string">'pointwise'</span><span class="token punctuation">,</span> pointwise<span class="token punctuation">)</span><span class="token punctuation">,</span>
                                                    <span class="token punctuation">(</span><span class="token string">'bn_point'</span><span class="token punctuation">,</span> bn_point<span class="token punctuation">)</span>
                                                    <span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>block <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>OrderedDict<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'depthwise'</span><span class="token punctuation">,</span> depthwise<span class="token punctuation">)</span><span class="token punctuation">,</span>
                                                    <span class="token punctuation">(</span><span class="token string">'bn_depth'</span><span class="token punctuation">,</span> bn_depth<span class="token punctuation">)</span><span class="token punctuation">,</span>
                                                    <span class="token punctuation">(</span><span class="token string">'relu1'</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                                    <span class="token punctuation">(</span><span class="token string">'pointwise'</span><span class="token punctuation">,</span> pointwise<span class="token punctuation">)</span><span class="token punctuation">,</span>
                                                    <span class="token punctuation">(</span><span class="token string">'bn_point'</span><span class="token punctuation">,</span> bn_point<span class="token punctuation">)</span><span class="token punctuation">,</span>
                                                    <span class="token punctuation">(</span><span class="token string">'relu2'</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
                                                    <span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>block<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br></div></div><h3 id="全局特征提取器-global-feature-extrator"><a href="#全局特征提取器-global-feature-extrator" class="header-anchor">#</a> 全局特征提取器(Global Feature Extrator)</h3> <p>全局特征提取器模块的目的在于捕获分割图像的全局上下文信息。该模块直接将下采样学习模块的结果(分辨率为原图的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mn>8</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac 18</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">8</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>)作为而输入。该模块引入了MobileNet-V2中提出的有效的瓶<strong>颈残差网络(efficient bottleneck residual blocks)</strong>。当输入的图像和输出的图像尺寸相同时，使用残差连接链接瓶颈残差块。</p> <p>在瓶颈残差块中使用了有效的深度可分离卷积，从而减少了参数量和浮点数运算。最后还添加了一个金字塔池化模块(pyramid pooling module 简称PPM)，用于汇总基于不同区域的上下文信息。</p> <p>在各层的详细参数如下表：</p> <p><img src="/assets/img/20210512135254image-20210512135253006.fa99356d.png" alt="image-20210512135253006"></p> <p>每一条横线分别表示，下采样学习模块，全局特征提取器，特征融合，分类四个总体模块</p> <p>其中t,c,n,s分别表示瓶颈块的拓展因子，输入通道数，使用该层的次数，步长</p> <p>瓶颈块的参数表：</p> <p><img src="/assets/img/20210512135639image-20210512135638073.9d6352f7.png" alt="image-20210512135638073"></p> <p>瓶颈残差块将输入为c的图像转化为具有拓展因子t的c`</p> <p>最后的点卷积不适用非线性函数f</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token comment"># 主网络声明</span>
self<span class="token punctuation">.</span>global_feature_extractor <span class="token operator">=</span> GlobalFeatureExtractor<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">96</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span>norm_layer<span class="token operator">=</span>self<span class="token punctuation">.</span>norm_layer<span class="token punctuation">)</span>

<span class="token comment"># 全局特征提取器对应的模块类</span>
<span class="token keyword">class</span> <span class="token class-name">GlobalFeatureExtractor</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;Global feature extractor module&quot;&quot;&quot;</span>
    <span class="token comment"># 输入的通道数，每一层的通道数，输出的通道数，拓展因子t，块在每一层的数量</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_channels<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> block_channels<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">96</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">,</span> out_channels<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">,</span>
                 t<span class="token operator">=</span><span class="token number">6</span><span class="token punctuation">,</span> num_blocks<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> norm_layer<span class="token operator">=</span>nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>GlobalFeatureExtractor<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 创建瓶颈残差块，这里使用的InvertedResidual叫做反向残差。</span>
        <span class="token comment"># 只有步长为1并且输入通道和输出通道相同的情况下这各个反向残差才会使用残差连接</span>
        self<span class="token punctuation">.</span>bottleneck1 <span class="token operator">=</span> self<span class="token punctuation">.</span>_make_layer<span class="token punctuation">(</span>InvertedResidual<span class="token punctuation">,</span> in_channels<span class="token punctuation">,</span> block_channels<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> num_blocks<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                                            t<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> norm_layer<span class="token operator">=</span>norm_layer<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>bottleneck2 <span class="token operator">=</span> self<span class="token punctuation">.</span>_make_layer<span class="token punctuation">(</span>InvertedResidual<span class="token punctuation">,</span> block_channels<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> block_channels<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                                            num_blocks<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> t<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> norm_layer<span class="token operator">=</span>norm_layer<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>bottleneck3 <span class="token operator">=</span> self<span class="token punctuation">.</span>_make_layer<span class="token punctuation">(</span>InvertedResidual<span class="token punctuation">,</span> block_channels<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> block_channels<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                                            num_blocks<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> t<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> norm_layer<span class="token operator">=</span>norm_layer<span class="token punctuation">)</span>
        <span class="token comment"># 做一个金字塔池化</span>
        self<span class="token punctuation">.</span>ppm <span class="token operator">=</span> PyramidPooling<span class="token punctuation">(</span>block_channels<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> norm_layer<span class="token operator">=</span>norm_layer<span class="token punctuation">)</span>
        <span class="token comment"># 最后使用1x1卷积输出成对应的通道，进行输出</span>
        self<span class="token punctuation">.</span>out <span class="token operator">=</span> _ConvBNReLU<span class="token punctuation">(</span>block_channels<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> norm_layer<span class="token operator">=</span>norm_layer<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">_make_layer</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> block<span class="token punctuation">,</span> inplanes<span class="token punctuation">,</span> planes<span class="token punctuation">,</span> blocks<span class="token punctuation">,</span> t<span class="token operator">=</span><span class="token number">6</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> norm_layer<span class="token operator">=</span>nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 使用的模块，输入的通道数，输出的通道数，块的数量，拓展因子t，步长</span>
        <span class="token comment"># 初始化一个容器</span>
        layers <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token comment"># 将块中的信息加入</span>
        layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>block<span class="token punctuation">(</span>inplanes<span class="token punctuation">,</span> planes<span class="token punctuation">,</span> stride<span class="token punctuation">,</span> t<span class="token punctuation">,</span> norm_layer<span class="token operator">=</span>norm_layer<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># 重复这个块对应次</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> blocks<span class="token punctuation">)</span><span class="token punctuation">:</span>
            layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>block<span class="token punctuation">(</span>planes<span class="token punctuation">,</span> planes<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> t<span class="token punctuation">,</span> norm_layer<span class="token operator">=</span>norm_layer<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment">#  将对应的内容放入Sequential容器中</span>
        <span class="token keyword">return</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span>layers<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>bottleneck1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>bottleneck2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>bottleneck3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>ppm<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>out<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br></div></div><div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token comment"># 反向卷积块</span>
<span class="token keyword">class</span> <span class="token class-name">InvertedResidual</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> stride<span class="token punctuation">,</span> expand_ratio<span class="token punctuation">,</span> dilation<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> norm_layer<span class="token operator">=</span>nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 参数：输入通道，输出通道，步长，拓展因子，空洞卷积，</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>InvertedResidual<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">assert</span> stride <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span>
        <span class="token comment"># 是否使用残差连接</span>
        self<span class="token punctuation">.</span>use_res_connect <span class="token operator">=</span> stride <span class="token operator">==</span> <span class="token number">1</span> <span class="token keyword">and</span> in_channels <span class="token operator">==</span> out_channels

        layers <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 中间的通道数，使用拓展因子*输入的通道数</span>
        inter_channels <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span><span class="token builtin">round</span><span class="token punctuation">(</span>in_channels <span class="token operator">*</span> expand_ratio<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> expand_ratio <span class="token operator">!=</span> <span class="token number">1</span><span class="token punctuation">:</span>
            <span class="token comment"># pw</span>
            <span class="token comment"># 先做一个标准卷积嘛，使用中间通道数作为输出,1x1卷积</span>
            layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>_ConvBNReLU<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> inter_channels<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> relu6<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> norm_layer<span class="token operator">=</span>norm_layer<span class="token punctuation">)</span><span class="token punctuation">)</span>
        layers<span class="token punctuation">.</span>extend<span class="token punctuation">(</span><span class="token punctuation">[</span>
            <span class="token comment"># dw 这里使用了分组卷积，但是实际上和普通的卷积没有什么区别，如果groups整好是输入通道数的一个因素，则输入的通道会被分成对应的组进行卷积</span>
            _ConvBNReLU<span class="token punctuation">(</span>inter_channels<span class="token punctuation">,</span> inter_channels<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token punctuation">,</span> dilation<span class="token punctuation">,</span> dilation<span class="token punctuation">,</span>
                        groups<span class="token operator">=</span>inter_channels<span class="token punctuation">,</span> relu6<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> norm_layer<span class="token operator">=</span>norm_layer<span class="token punctuation">)</span><span class="token punctuation">,</span>
            <span class="token comment"># pw-linear</span>
            <span class="token comment"># 使用1x1卷积将中间通道数转化成最终的通道数</span>
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>inter_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            norm_layer<span class="token punctuation">(</span>out_channels<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span>layers<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 残差连接</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>use_res_connect<span class="token punctuation">:</span>
            <span class="token keyword">return</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>conv<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token keyword">return</span> self<span class="token punctuation">.</span>conv<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br></div></div><div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token comment"># PPM(金字塔池化模块)</span>
<span class="token keyword">class</span> <span class="token class-name">PyramidPooling</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_channels<span class="token punctuation">,</span> sizes<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">,</span> norm_layer<span class="token operator">=</span>nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>PyramidPooling<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 定义输出为输入的四分之一</span>
        out_channels <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>in_channels <span class="token operator">/</span> <span class="token number">4</span><span class="token punctuation">)</span>
        <span class="token comment"># 创建平均池化和卷积层模块列表</span>
        self<span class="token punctuation">.</span>avgpools <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>convs <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 遍历平均池化的尺寸</span>
        <span class="token keyword">for</span> size <span class="token keyword">in</span> sizes<span class="token punctuation">:</span>
            <span class="token comment"># 使用自适应平均池化，这里的参数，表示经过自适应平均池化的特征图输入为c X h X w，出来的结果为c X size X size</span>
            self<span class="token punctuation">.</span>avgpools<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>AdaptiveAvgPool2d<span class="token punctuation">(</span>size<span class="token punctuation">)</span><span class="token punctuation">)</span>
            <span class="token comment"># 使用普通卷积层进行卷积1x1的卷积核</span>
            self<span class="token punctuation">.</span>convs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>_ConvBNReLU<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> norm_layer<span class="token operator">=</span>norm_layer<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        size <span class="token operator">=</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">]</span>
        feats <span class="token operator">=</span> <span class="token punctuation">[</span>x<span class="token punctuation">]</span>
        <span class="token keyword">for</span> <span class="token punctuation">(</span>avgpool<span class="token punctuation">,</span> conv<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>avgpools<span class="token punctuation">,</span> self<span class="token punctuation">.</span>convs<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># 没记错的话interpolate应该是上采样到size的大小</span>
            feats<span class="token punctuation">.</span>append<span class="token punctuation">(</span>F<span class="token punctuation">.</span>interpolate<span class="token punctuation">(</span>conv<span class="token punctuation">(</span>avgpool<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> size<span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'bilinear'</span><span class="token punctuation">,</span> align_corners<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># 记录完平均池化的结果后，就进行拼接</span>
        <span class="token keyword">return</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>feats<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br></div></div><h3 id="特征融合模块-feature-fusion-module"><a href="#特征融合模块-feature-fusion-module" class="header-anchor">#</a> 特征融合模块(Feature Fusion Module)</h3> <p><img src="/assets/img/20210512140504image-20210512140501709.035ebece.png" alt="image-20210512140501709"></p> <p>先前下采样学习模块计算的特征图（表的左边）只经过一个<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">1 \times 1</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>的卷积即可，在x次下采样后的结果(经过全局特征提取模块的特征图，表的右边)，上采样X次，使用可分离卷积和一个非线性函数，再使用一个<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">1 \times1</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>的卷积，最后将两个特征图加起来，再使用非线性激活函数f</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token comment"># 在主类中的声明：</span>
self<span class="token punctuation">.</span>feature_fusion <span class="token operator">=</span> FeatureFusionModule<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> norm_layer<span class="token operator">=</span>self<span class="token punctuation">.</span>norm_layer<span class="token punctuation">)</span>

<span class="token comment"># 特征融合模块</span>
<span class="token keyword">class</span> <span class="token class-name">FeatureFusionModule</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;Feature fusion module&quot;&quot;&quot;</span>
    <span class="token comment"># 输入的参数为高输入的通道数，低输入的通道数，输出的通道数</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> highter_in_channels<span class="token punctuation">,</span> lower_in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> scale_factor<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span> norm_layer<span class="token operator">=</span>nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>FeatureFusionModule<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 设置规模</span>
        self<span class="token punctuation">.</span>scale_factor <span class="token operator">=</span> scale_factor
        <span class="token comment"># 使用普通卷积将低通道数转化成输出的通道数</span>
        self<span class="token punctuation">.</span>dwconv <span class="token operator">=</span> _ConvBNReLU<span class="token punctuation">(</span>lower_in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> norm_layer<span class="token operator">=</span>norm_layer<span class="token punctuation">)</span>
        <span class="token comment"># 再对低维卷积的将诶过再做一个1x1卷积，但是不激活</span>
        self<span class="token punctuation">.</span>conv_lower_res <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>out_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            norm_layer<span class="token punctuation">(</span>out_channels<span class="token punctuation">)</span>
        <span class="token punctuation">)</span>
        <span class="token comment"># 对高维度的卷积，只使用1x1卷积，不使用激活函数</span>
        self<span class="token punctuation">.</span>conv_higher_res <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>highter_in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            norm_layer<span class="token punctuation">(</span>out_channels<span class="token punctuation">)</span>
        <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>relu <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> higher_res_feature<span class="token punctuation">,</span> lower_res_feature<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 先将低维特征图上采样到现在的4倍</span>
        lower_res_feature <span class="token operator">=</span> F<span class="token punctuation">.</span>interpolate<span class="token punctuation">(</span>lower_res_feature<span class="token punctuation">,</span> scale_factor<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'bilinear'</span><span class="token punctuation">,</span> align_corners<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        <span class="token comment"># 将低纬度的通道数转化成输出的通道数</span>
        lower_res_feature <span class="token operator">=</span> self<span class="token punctuation">.</span>dwconv<span class="token punctuation">(</span>lower_res_feature<span class="token punctuation">)</span>
        <span class="token comment"># 再做一次1x1卷积，但是不激活</span>
        lower_res_feature <span class="token operator">=</span> self<span class="token punctuation">.</span>conv_lower_res<span class="token punctuation">(</span>lower_res_feature<span class="token punctuation">)</span>
        <span class="token comment"># 对高纬度进行1x1卷积，但是不激活</span>
        higher_res_feature <span class="token operator">=</span> self<span class="token punctuation">.</span>conv_higher_res<span class="token punctuation">(</span>higher_res_feature<span class="token punctuation">)</span>
        <span class="token comment"># 将低纬度和高纬度加起来</span>
        out <span class="token operator">=</span> higher_res_feature <span class="token operator">+</span> lower_res_feature
        <span class="token comment"># 最后激活他就行</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>out<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br></div></div><h3 id="分类模块-classifier"><a href="#分类模块-classifier" class="header-anchor">#</a> 分类模块(classifier)</h3> <p>在分类模块中采用两个深度可分离卷积(DSConv)和一个普通卷积(Conv2D，纠正一下，之前说过的点卷积是Conv2D)。</p> <p>为了适应梯度下降，所以在训练中使用了Softmax激活函数，在推理过程中,由于argmax和sorftmax都是单调递增的函数，所以使用argmax代替softmax减小计算开销。</p> <p>如果需要Fast-SCNN的概率模型，才在推理时使用softmax。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token comment"># 在主类中的声明：</span>
self<span class="token punctuation">.</span>classifier <span class="token operator">=</span> Classifer<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>nclass<span class="token punctuation">,</span> norm_layer<span class="token operator">=</span>self<span class="token punctuation">.</span>norm_layer<span class="token punctuation">)</span>

<span class="token comment"># 分类模块</span>
<span class="token keyword">class</span> <span class="token class-name">Classifer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;Classifer&quot;&quot;&quot;</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dw_channels<span class="token punctuation">,</span> num_classes<span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> norm_layer<span class="token operator">=</span>nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 参数：输入的通道数，分类数，步长，BN</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Classifer<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 使用2个深度分离卷积</span>
        self<span class="token punctuation">.</span>dsconv1 <span class="token operator">=</span> SeparableConv2d<span class="token punctuation">(</span>dw_channels<span class="token punctuation">,</span> dw_channels<span class="token punctuation">,</span> stride<span class="token operator">=</span>stride<span class="token punctuation">,</span> relu_first<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
                                       norm_layer<span class="token operator">=</span>norm_layer<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dsconv2 <span class="token operator">=</span> SeparableConv2d<span class="token punctuation">(</span>dw_channels<span class="token punctuation">,</span> dw_channels<span class="token punctuation">,</span> stride<span class="token operator">=</span>stride<span class="token punctuation">,</span> relu_first<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
                                       norm_layer<span class="token operator">=</span>norm_layer<span class="token punctuation">)</span>
        <span class="token comment"># 设置随机失活(dropout2d)，然后进行卷积，不适用BN不使用，激活，使用1x1卷积</span>
        self<span class="token punctuation">.</span>conv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Dropout2d<span class="token punctuation">(</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>dw_channels<span class="token punctuation">,</span> num_classes<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>dsconv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>dsconv2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x

</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br></div></div></div> <footer class="page-edit"><!----> <div class="last-updated"><span class="prefix">上次更新时间:</span> <span class="time">2021/7/27 上午4:27:40</span></div></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/unlimited-paper-works/[12]MobileNetV2-Inverted-Residuals-and-Linear-bottleneck.html" class="prev">
        MobileNetV2: Inverted Residuals and Linear Bottlenecks
      </a></span> <span class="next"><a href="/unlimited-paper-works/[14]MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications.html">
        MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications
      </a>
      →
    </span></p></div> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.0cf0d483.js" defer></script><script src="/assets/js/2.d537feee.js" defer></script><script src="/assets/js/28.35498a04.js" defer></script>
  </body>
</html>
