<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>A Review on Deep Learning Techniques Applied to Semantic Segmentation | 工具箱的深度学习记事簿</title>
    <meta name="generator" content="VuePress 1.8.2">
    <link rel="icon" href="/statics/logo.svg">
    <meta name="description" content="这里包含了我从入门到依然在入门的过程中接触到的大部分知识。翻翻目录，也许能找到有用的">
    <meta name="keywords" content="Akasaki,Deep learning,Machine learning,工具箱,工具箱的深度学习记事簿,Akasaki的深度学习记事簿">
    <meta name="google-site-verification" content="VVNYs0bXM_EKTgxJ8XIfvXShjHsksGNv3YNedxBGFjU">
    
    <link rel="preload" href="/assets/css/0.styles.b2187a0e.css" as="style"><link rel="preload" href="/assets/js/app.0cf0d483.js" as="script"><link rel="preload" href="/assets/js/2.d537feee.js" as="script"><link rel="preload" href="/assets/js/6.59e5684c.js" as="script"><link rel="prefetch" href="/assets/js/10.000b9996.js"><link rel="prefetch" href="/assets/js/100.01b72e43.js"><link rel="prefetch" href="/assets/js/101.ab99304f.js"><link rel="prefetch" href="/assets/js/102.b1ad1ca3.js"><link rel="prefetch" href="/assets/js/11.0ec32f4a.js"><link rel="prefetch" href="/assets/js/12.7ad41a4b.js"><link rel="prefetch" href="/assets/js/13.40c87056.js"><link rel="prefetch" href="/assets/js/14.2d8bcb8d.js"><link rel="prefetch" href="/assets/js/15.a7295b50.js"><link rel="prefetch" href="/assets/js/16.7158147a.js"><link rel="prefetch" href="/assets/js/17.9215c05c.js"><link rel="prefetch" href="/assets/js/18.582ac004.js"><link rel="prefetch" href="/assets/js/19.7cc8fc31.js"><link rel="prefetch" href="/assets/js/20.1b1e2af4.js"><link rel="prefetch" href="/assets/js/21.b2dadf61.js"><link rel="prefetch" href="/assets/js/22.b19748fc.js"><link rel="prefetch" href="/assets/js/23.f2db4fa1.js"><link rel="prefetch" href="/assets/js/24.3c31aece.js"><link rel="prefetch" href="/assets/js/25.531fd9fb.js"><link rel="prefetch" href="/assets/js/26.6e841782.js"><link rel="prefetch" href="/assets/js/27.33add335.js"><link rel="prefetch" href="/assets/js/28.35498a04.js"><link rel="prefetch" href="/assets/js/29.aabed81c.js"><link rel="prefetch" href="/assets/js/3.0a121536.js"><link rel="prefetch" href="/assets/js/30.54f44e16.js"><link rel="prefetch" href="/assets/js/31.8ecbe721.js"><link rel="prefetch" href="/assets/js/32.873f263a.js"><link rel="prefetch" href="/assets/js/33.106c15c8.js"><link rel="prefetch" href="/assets/js/34.349d6fdb.js"><link rel="prefetch" href="/assets/js/35.73569c69.js"><link rel="prefetch" href="/assets/js/36.24f08107.js"><link rel="prefetch" href="/assets/js/37.da460b69.js"><link rel="prefetch" href="/assets/js/38.2f6c4646.js"><link rel="prefetch" href="/assets/js/39.435f8032.js"><link rel="prefetch" href="/assets/js/4.972cd905.js"><link rel="prefetch" href="/assets/js/40.4760c47b.js"><link rel="prefetch" href="/assets/js/41.a7cc5e5d.js"><link rel="prefetch" href="/assets/js/42.fc3e47f2.js"><link rel="prefetch" href="/assets/js/43.a13c67d4.js"><link rel="prefetch" href="/assets/js/44.46c317cb.js"><link rel="prefetch" href="/assets/js/45.f11cdfc6.js"><link rel="prefetch" href="/assets/js/46.541bd27d.js"><link rel="prefetch" href="/assets/js/47.5705cba6.js"><link rel="prefetch" href="/assets/js/48.d2040c65.js"><link rel="prefetch" href="/assets/js/49.cbb5c42b.js"><link rel="prefetch" href="/assets/js/5.e2cb1c20.js"><link rel="prefetch" href="/assets/js/50.39732f79.js"><link rel="prefetch" href="/assets/js/51.99b82adb.js"><link rel="prefetch" href="/assets/js/52.808c9ece.js"><link rel="prefetch" href="/assets/js/53.67e417da.js"><link rel="prefetch" href="/assets/js/54.1c722ed6.js"><link rel="prefetch" href="/assets/js/55.afc1aceb.js"><link rel="prefetch" href="/assets/js/56.a463c15a.js"><link rel="prefetch" href="/assets/js/57.f90cb51c.js"><link rel="prefetch" href="/assets/js/58.8e11cf82.js"><link rel="prefetch" href="/assets/js/59.ed4fc789.js"><link rel="prefetch" href="/assets/js/60.adcefa1d.js"><link rel="prefetch" href="/assets/js/61.a6446644.js"><link rel="prefetch" href="/assets/js/62.1721a24b.js"><link rel="prefetch" href="/assets/js/63.726a6135.js"><link rel="prefetch" href="/assets/js/64.e2db755f.js"><link rel="prefetch" href="/assets/js/65.e92b9d46.js"><link rel="prefetch" href="/assets/js/66.b4d3e87e.js"><link rel="prefetch" href="/assets/js/67.5f3eaf29.js"><link rel="prefetch" href="/assets/js/68.85b4615c.js"><link rel="prefetch" href="/assets/js/69.63d409f1.js"><link rel="prefetch" href="/assets/js/7.b3ddb4c6.js"><link rel="prefetch" href="/assets/js/70.6f3f7332.js"><link rel="prefetch" href="/assets/js/71.0a087c8a.js"><link rel="prefetch" href="/assets/js/72.6513570f.js"><link rel="prefetch" href="/assets/js/73.896f4d9c.js"><link rel="prefetch" href="/assets/js/74.ecc3d286.js"><link rel="prefetch" href="/assets/js/75.b06a8964.js"><link rel="prefetch" href="/assets/js/76.3032add1.js"><link rel="prefetch" href="/assets/js/77.4fb4de51.js"><link rel="prefetch" href="/assets/js/78.42cf5412.js"><link rel="prefetch" href="/assets/js/79.0ba88830.js"><link rel="prefetch" href="/assets/js/8.bb97c23d.js"><link rel="prefetch" href="/assets/js/80.aaa6656c.js"><link rel="prefetch" href="/assets/js/81.4a1804f1.js"><link rel="prefetch" href="/assets/js/82.7ad53f0e.js"><link rel="prefetch" href="/assets/js/83.c4173111.js"><link rel="prefetch" href="/assets/js/84.cb7300ea.js"><link rel="prefetch" href="/assets/js/85.81c0ea84.js"><link rel="prefetch" href="/assets/js/86.317fcc1c.js"><link rel="prefetch" href="/assets/js/87.6f50a42f.js"><link rel="prefetch" href="/assets/js/88.898a4cd8.js"><link rel="prefetch" href="/assets/js/89.ddbe5c5c.js"><link rel="prefetch" href="/assets/js/9.596e7f45.js"><link rel="prefetch" href="/assets/js/90.c1e40160.js"><link rel="prefetch" href="/assets/js/91.f85fdec8.js"><link rel="prefetch" href="/assets/js/92.1d7822a2.js"><link rel="prefetch" href="/assets/js/93.4ee2ef21.js"><link rel="prefetch" href="/assets/js/94.3760a5b4.js"><link rel="prefetch" href="/assets/js/95.b5be500a.js"><link rel="prefetch" href="/assets/js/96.e570f48e.js"><link rel="prefetch" href="/assets/js/97.af175cfb.js"><link rel="prefetch" href="/assets/js/98.ca3ecf81.js"><link rel="prefetch" href="/assets/js/99.2fc27711.js">
    <link rel="stylesheet" href="/assets/css/0.styles.b2187a0e.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><!----> <span class="site-name">工具箱的深度学习记事簿</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="https://github.com/VisualDust/ml.akasaki.space" target="_blank" rel="noopener noreferrer" class="nav-link external">
  View on Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div><div class="nav-item"><a href="https://github.com/VisualDust" target="_blank" rel="noopener noreferrer" class="nav-link external">
  工具箱
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="https://github.com/VisualDust/ml.akasaki.space" target="_blank" rel="noopener noreferrer" class="nav-link external">
  View on Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div><div class="nav-item"><a href="https://github.com/VisualDust" target="_blank" rel="noopener noreferrer" class="nav-link external">
  工具箱
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第零章：在开始之前</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第一章上：HelloWorld</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第一章下：深度学习基础</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第二章上：卷积神经网络</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第二章下：经典卷积神经网络</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第三章上：谈一些计算机视觉方向</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>第三章下：了解更高级的技术</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>魔法部日志（又名论文阅读日志）</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/unlimited-paper-works/[0]unlimited-paper-works.html" class="sidebar-link">欢迎来到魔法部日志</a></li><li><a href="/unlimited-paper-works/[1]The-Devil-is-in-the-Decoder-Classification-Regression-and-GANs.html" class="sidebar-link">The Devil is in the Decoder: Classification, Regression and GANs</a></li><li><a href="/unlimited-paper-works/[2]Threat-of-Adversarial-Attacks-on-Deep-Learning-in-Computer-Vision-A-Survey.html" class="sidebar-link">Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey</a></li><li><a href="/unlimited-paper-works/[3]Progressive-Semantic-Segmentation.html" class="sidebar-link">Progressive Semantic Segmentation</a></li><li><a href="/unlimited-paper-works/[4]Decoders-Matter-for-Semantic-Segmentation-Data-Dependent-Decoding-Enables-Flexible-Feature-Aggregation.html" class="sidebar-link">Decoders Matter for Semantic Segmentation: Data-Dependent Decoding Enables Flexible Feature Aggregation</a></li><li><a href="/unlimited-paper-works/[5]HLA-Face-Joint-High-Low-Adaptation-for-Low-Light-Face-Detection.html" class="sidebar-link">HLA-Face Joint High-Low Adaptation for Low Light Face Detection</a></li><li><a href="/unlimited-paper-works/[6]DeepLab-Series.html" class="sidebar-link">DeepLab Series</a></li><li><a href="/unlimited-paper-works/[7]Cross-Dataset-Collaborative-Learning-for-Semantic-Segmentation.html" class="sidebar-link">Cross-Dataset Collaborative Learning for Semantic Segmentation</a></li><li><a href="/unlimited-paper-works/[8]Dynamic-Neural-Networks-A-Survey.html" class="sidebar-link">Dynamic Neural Networks: A Survey</a></li><li><a href="/unlimited-paper-works/[9]Feature-Pyramid-Networks-for-Object-Detection.html" class="sidebar-link">Feature Pyramid Networks for Object Detection</a></li><li><a href="/unlimited-paper-works/[10]Overview-Of-Semantic-Segmentation.html" class="active sidebar-link">A Review on Deep Learning Techniques Applied to Semantic Segmentation</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[10]Overview-Of-Semantic-Segmentation.html#介绍分割" class="sidebar-link">介绍分割</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[10]Overview-Of-Semantic-Segmentation.html#分割的技术" class="sidebar-link">分割的技术</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[10]Overview-Of-Semantic-Segmentation.html#块分类-patch-classification" class="sidebar-link">块分类（Patch classification）</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[10]Overview-Of-Semantic-Segmentation.html#全卷积方法-基于fcn" class="sidebar-link">全卷积方法（基于FCN）</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[10]Overview-Of-Semantic-Segmentation.html#编码器-解码器结构-encoder-decoder-本质基于fcn" class="sidebar-link">编码器-解码器结构（encoder-decoder，本质基于FCN）</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[10]Overview-Of-Semantic-Segmentation.html#空洞卷积-dilated-atrous-convolution-代替了-池化-上采样-的过程" class="sidebar-link">空洞卷积（Dilated/Atrous Convolution，代替了“池化-上采样”的过程）</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[10]Overview-Of-Semantic-Segmentation.html#条件随机场" class="sidebar-link">条件随机场</a></li></ul></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[10]Overview-Of-Semantic-Segmentation.html#分割的数据集" class="sidebar-link">分割的数据集</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[10]Overview-Of-Semantic-Segmentation.html#领域知名论文" class="sidebar-link">领域知名论文</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[10]Overview-Of-Semantic-Segmentation.html#基于深度学习的分割方法" class="sidebar-link">基于深度学习的分割方法</a></li><li class="sidebar-sub-header"><a href="/unlimited-paper-works/[10]Overview-Of-Semantic-Segmentation.html#上述方法的关系" class="sidebar-link">上述方法的关系</a></li></ul></li></ul></li><li><a href="/unlimited-paper-works/[11]Image-Segmentation-Using-Deep-Learning-A-Survey.html" class="sidebar-link">Image Segmentation Using Deep Learning: A Survey</a></li><li><a href="/unlimited-paper-works/[12]MobileNetV2-Inverted-Residuals-and-Linear-bottleneck.html" class="sidebar-link">MobileNetV2: Inverted Residuals and Linear Bottlenecks</a></li><li><a href="/unlimited-paper-works/[13]Fast-SCNN-Fast-Semantic-Segmentation-Network.html" class="sidebar-link">Fast-SCNN: Fast Semantic Segmentation Network</a></li><li><a href="/unlimited-paper-works/[14]MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications.html" class="sidebar-link">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</a></li><li><a href="/unlimited-paper-works/[15]Gated-Channel-Transformation-for-Visual-Recognition.html" class="sidebar-link">Gated Channel Transformation for Visual Recognition</a></li><li><a href="/unlimited-paper-works/[16]Convolutional-Block-Attention-Module.html" class="sidebar-link">Convolutional Block Attention Module</a></li><li><a href="/unlimited-paper-works/[17]Boundary-IoU-Improving-Object-Centri Image-Segmentation-Evaluation.html" class="sidebar-link">Boundary IoU: Improving Object-Centric Image Segmentation Evaluation</a></li><li><a href="/unlimited-paper-works/[18]Involution-Inverting-the-Inherence-of-Convolution-for-Visual-Recognition.html" class="sidebar-link">Involution: Inverting the Inherence of Convolution for Visual Recognition</a></li><li><a href="/unlimited-paper-works/[19]PointRend-Image-Segmentation-as-Rendering.html" class="sidebar-link">PointRend: Image Segmentation as Rendering</a></li><li><a href="/unlimited-paper-works/[20]Transformer-Attention-is-all-you-need.html" class="sidebar-link">Transformer: Attention is all you need</a></li><li><a href="/unlimited-paper-works/[21]RefineMask_Towards_High-Quality_Instance_Segmentationwith_Fine-Grained_Features.html" class="sidebar-link">RefineMask: Towards High-Quality Instance Segmentationwith Fine-Grained Features</a></li><li><a href="/unlimited-paper-works/[22]GLADNet-Low-Light-Enhancement-Network-with-Global-Awareness.html" class="sidebar-link">GLADNet: Low-Light Enhancement Network with Global Awareness</a></li><li><a href="/unlimited-paper-works/[23]Squeeze-and-Excitation-Networks.html" class="sidebar-link">Squeeze-and-Excitation Networks (SENet)</a></li><li><a href="/unlimited-paper-works/[24]BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation.html" class="sidebar-link">BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation</a></li><li><a href="/unlimited-paper-works/[25]Rethinking-BiSeNet-For-Real-time-Semantic-Segmentation.html" class="sidebar-link">Rethinking BiSeNet For Real-time Semantic Segmentation</a></li><li><a href="/unlimited-paper-works/[26]CBAM-Convolutional-Block-Attention-Module.html" class="sidebar-link">CBAM: Convolutional Block Attention Module</a></li><li><a href="/unlimited-paper-works/[27]Non-local-Neural-Networks.html" class="sidebar-link">Non-local Neural Networks</a></li><li><a href="/unlimited-paper-works/[28]GCNet-Non-local-Networks-Meet-Squeeze-Excitation-Networks-and-Beyond.html" class="sidebar-link">GCNet: Global Context Networks (Non-local Networks Meet Squeeze-Excitation Networks and Beyond)</a></li><li><a href="/unlimited-paper-works/[29]Disentangled-Non-Local-Neural-Networks.html" class="sidebar-link">Disentangled Non-Local Neural Networks</a></li><li><a href="/unlimited-paper-works/[30]RetinexNet-for-Low-Light-Enhancement.html" class="sidebar-link">Deep Retinex Decomposition for Low-Light Enhancement</a></li><li><a href="/unlimited-paper-works/[31]MSR-netLow-light-Image-Enhancement-Using-Deep-Convolutional-Network.html" class="sidebar-link">MSR-net:Low-light Image Enhancement Using Deep Convolutional Network</a></li><li><a href="/unlimited-paper-works/[32]LLCNN-A-Convolutional-Neural-Network-for-Low-light-Image-Enhancement.html" class="sidebar-link">LLCNN: A convolutional neural network for low-light image enhancement</a></li></ul></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>附录1：好朋友们</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>附录2：数学是真正的圣经</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>附录3：信号和采样的学问（DSP）</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>附录4：TensorFlow编程策略</span> <span class="arrow right"></span></p> <!----></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="a-review-on-deep-learning-techniques-applied-to-semantic-segmentation"><a href="#a-review-on-deep-learning-techniques-applied-to-semantic-segmentation" class="header-anchor">#</a> A Review on Deep Learning Techniques Applied to Semantic Segmentation</h1> <h3 id="这篇笔记的写作者是visualdust。"><a href="#这篇笔记的写作者是visualdust。" class="header-anchor">#</a> 这篇笔记的写作者是<a href="https://github.com/visualDust" target="_blank" rel="noopener noreferrer">VisualDust<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>。</h3> <p>这是一篇关于综述论文的解读。<a href="https://arxiv.org/pdf/1704.06857.pdf" target="_blank" rel="noopener noreferrer">原论文（A Review on Deep Learning Techniques Applied to Semantic Segmentation）<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p>摘要：</p> <blockquote><p>Image semantic segmentation is more and more being of interest for computer vision and machine learning researchers. Many applications on the rise need accurate and efficient segmentation mechanisms: autonomous driving, indoor navigation, and even virtual or augmented reality systems to name a few. This demand coincides with the rise of deep learning approaches in almost every field or application target related to computer vision, including semantic segmentation or scene understanding. This paper provides a review on deep learning methods for semantic segmentation applied to various application areas. Firstly, we describe the terminology of this field as well as mandatory background concepts. Next, the main datasets and challenges are exposed to help researchers decide which are the ones that best suit their needs and their targets. Then, existing methods are reviewed, highlighting their contributions and their significance in the field. Finally, quantitative results are given for the described methods and the datasets in which they were evaluated, following up with a discussion of the results. At last, we point out a set of promising future works and draw our own conclusions about the state of the art of semantic segmentation using deep learning techniques.</p></blockquote> <p>我看了这篇综述受益匪浅，如果有时间的话请阅读<a href="https://arxiv.org/pdf/1704.06857.pdf" target="_blank" rel="noopener noreferrer">原作<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>。本文只是对原作阅读的粗浅笔记。</p> <h2 id="介绍分割"><a href="#介绍分割" class="header-anchor">#</a> 介绍分割</h2> <p>对图像进行分割主要有：语义分割（Semantic segmentation）和实例分割（Instance segmentation）。它们的区别一目了然：</p> <p><img src="/assets/img/image-20210427154733807.fbb63248.png" alt="image-20210427154733807"></p> <p>左图：原图；中图：语义分割；右图：实例分割。</p> <p>很明显，语义分割希望将不同类别的物体所在位置的像素分开来，但是对于相同类别的不同物体并不敏感；而实例分割不但需要分开每一个位置上像素属于哪一类，还要分出它具体属于哪一个对象。</p> <p>我们知道一个图像只不过是许多像素的集合。图像分割分类是对图像中属于特定类别的像素进行分类的过程，因此<strong>图像分割可以认为是按像素进行分类的问题</strong>。</p> <p>如果你对离散数学以及softmax很敏感的化，肯定第一时间会产生这样的联想：</p> <p><img src="/assets/img/image-20210427222245438.bc6da1e9.png" alt="image-20210427222245438"></p> <p>这张图实际上是这样的：</p> <p><img src="/assets/img/image-20210427222340602.ecd1ca30.png" alt="image-20210427222340602"></p> <p>当然，对于实际应用中通道数量的具体数字可根据实际需求选择。例如，在前景分割中，仅需分割出前景和背景，因此只需要一个通道。而全景分割中，如果使用类one-hot编码，则需要有和对象数目+1一样多的通道数。</p> <h2 id="分割的技术"><a href="#分割的技术" class="header-anchor">#</a> 分割的技术</h2> <p>在深度学习方法流行之前，TextonForest和基于随机森林分类器等语义分割方法是用得比较多的方法。但是本文章的背景是基于深度学习方法的计算机视觉，所以不做过多讨论。</p> <p>深度学习技术在各个计算机领域获得了巨大的成功，其解决语义分割问题可以概括为几种思路：</p> <ul><li>块分类（Patch classification）</li> <li>全卷积方法（基于FCN）</li> <li>编码器-解码器结构（encoder-decoder，本质基于FCN）</li> <li>跨层连接的encoder-decoder结构</li></ul> <h3 id="块分类-patch-classification"><a href="#块分类-patch-classification" class="header-anchor">#</a> 块分类（Patch classification）</h3> <p>块分类算得上是一类最古老的方法。</p> <p>如其名，把图像分成小块塞给网络进行分类。分成指定大小的小块是因为全连接网络只接受指定大小的输入。这大概是最初的基于深度学习的分割方法了（吧）。</p> <h3 id="全卷积方法-基于fcn"><a href="#全卷积方法-基于fcn" class="header-anchor">#</a> 全卷积方法（基于FCN）</h3> <p>全卷积方法在块分类之后，优势是使用全卷积代替了块分类中的全连接。</p> <p>用于代替全连接的全卷积方法除了在其他视觉方法里很出名，也很快用到了分割算法中。2014年，全卷积网络（FCN）横空出世，FCN将网络全连接层用卷积取代，因此使任意图像大小的输入都变成可能，而且速度比Patch classification方法快很多。（我用简单分类模型实测了一下也是，全连接真的是太烂了，又慢又重，但是作为多层感知机到全卷积网路中间的过度组件，还是功不可没的。）</p> <h4 id="插值法实现的上采样"><a href="#插值法实现的上采样" class="header-anchor">#</a> 插值法实现的上采样</h4> <p>在全卷积方法中，为了使输出和输入大小相同，在卷积导致特征图变小后还需要经过上采样使特征图变为原来大小。</p> <p><img src="/assets/img/deconv01.e0912566.gif" alt="deconv01"></p> <p>上图：一种反卷积的示意。其中蓝色较小的特征图是输入，通过在它周围填充，使其变为较大的特征图后，再进行卷积。得到的结果是绿色的特征图。</p> <p><img src="/assets/img/deconv02.a2afa892.gif" alt="deconv02"></p> <p>上图：另一种反卷积的示意。其中蓝色较小的特征图经过某种填充方法进行填充，变为较大的特征图后再进行卷积。</p> <p>反卷积的常见思路是通过一些填充的方法将较小的特征图变大，然后通过卷积获得比原来的小特征图更大的特征图。较为常用的填充方法是插值法。</p> <p>插值的方法主要可以分为两类，一类是线性图像插值方法：</p> <ul><li>最近邻插值(Nearest neighbor interpolation)</li> <li>双线性插值(Bi-Linear interpolation)</li> <li>双立方插值(Bi-Cubic interpolation)</li></ul> <p>另一类是非线性图像插值方法：</p> <ul><li>基于小波变换的插值算法</li> <li>基于边缘信息的插值算法。</li></ul> <p>以上的这些方法都是一些插值方法，需要我们在决定网络结构的时候进行挑选。这些方法就像是人工特征工程一样，并没有给神经网络学习的余地，神经网络不能自己学习如何更好地进行插值，这个显然是不够理想的。</p> <h4 id="转置卷积实现的上采样"><a href="#转置卷积实现的上采样" class="header-anchor">#</a> 转置卷积实现的上采样</h4> <p>在上采样的方法中，比较出名的是转置卷积，因为它允许我们使用可学习的上采样过程。</p> <p>典型的转置卷积运算将采用滤波器视图中当前值的点积并作为相应的输出位置产生的单个值，而转置卷积的过程基本想法。对于转置卷积，我们从低分辨率特征图中获取单个值，并将滤波器中的所有权重乘以该值，将加权值输出到更大的特征图。</p> <p><img src="/assets/img/image-20210427223356560.9b8f47d5.png" alt="image-20210427223356560"></p> <p>上图：转置卷积的一种示意。</p> <blockquote><p>Tips：神经网络中的解卷积层也被称作：转置卷积(Transposed Convolution)、上卷积（upconvolution）、完全卷积（full convolution）、转置卷积（transposed convolution）、微步卷积（fractionally-strided convolution）。</p> <p>转置卷积常常在一些文献中也称之为反卷积(Deconvolution)和部分跨越卷积(Fractionally-strided Convolution)，因为称之为反卷积容易让人以为和数字信号处理中反卷积混起来，造成不必要的误解，因此下文都将称为转置卷积，并且建议各位不要采用反卷积这个称呼。</p></blockquote> <h3 id="编码器-解码器结构-encoder-decoder-本质基于fcn"><a href="#编码器-解码器结构-encoder-decoder-本质基于fcn" class="header-anchor">#</a> 编码器-解码器结构（encoder-decoder，本质基于FCN）</h3> <p>encoder由于pooling逐渐减少空间维度，而decoder逐渐恢复空间维度和细节信息。</p> <p><img src="/assets/img/image-20210428220457279.eeac2965.png" alt="image-20210428220457279"></p> <p>实际上，符合下采样提取特征，再上采样恢复原大小的都可以称为encoder-decoder结构。</p> <h4 id="跨层连接的encoder-decoder结构"><a href="#跨层连接的encoder-decoder结构" class="header-anchor">#</a> 跨层连接的encoder-decoder结构</h4> <p>通常从encoder到decoder还有shortcut connetction（捷径连接，也就是跨层连接，其思想我猜是从VGG跨层连接出现的思想）。</p> <p><img src="/assets/img/image-20210427221642324.3385c8c2.png" alt="image-20210427221642324"></p> <p>上图是带有跨层连接的encoder-decoder的代表之一：UNet的结构。</p> <h4 id="高低层特征融合"><a href="#高低层特征融合" class="header-anchor">#</a> 高低层特征融合</h4> <p>由于池化操作造成的信息损失，上采样（即使采用解卷积操作）只能生成粗略的分割结果图。因此，论文从高分辨率的特征图中引入跳跃连接（shortcut/skip connection）操作改善上采样的精细程度（感觉像是从ResNet开始出现的思想）：</p> <p><img src="/assets/img/FCN-2.2e5c107f.png" alt="FCN-2"></p> <p>实验表明，这样的分割结果更细致更准确。在逐层fusion的过程中，做到第三行再往下，结果又会变差，所以作者做到这里就停了。可以看到如上三行的对应的结果：</p> <p><img src="/assets/img/FCN-3.c007e71d.png" alt="FCN-3"></p> <h3 id="空洞卷积-dilated-atrous-convolution-代替了-池化-上采样-的过程"><a href="#空洞卷积-dilated-atrous-convolution-代替了-池化-上采样-的过程" class="header-anchor">#</a> 空洞卷积（Dilated/Atrous Convolution，代替了“池化-上采样”的过程）</h3> <p>尽管FCN及encoder-decoder结构中移除了全连接层，但是CNN模型用于语义分割还存在一个问题，就是下采样操作。这里使用池化的下采样为例：pooling操作可以扩大感受野因而能够很好地整合上下文信息（context中文称为语境或者上下文，通俗的理解就是综合了更多的信息来进行决策），对high-level的任务（比如分类），这是很有效的。但同时，由于pooling下采样操作，使得分辨率降低，因此削弱了位置信息，而语义分割中需要score map和原图对齐，因此需要丰富的位置信息。</p> <p>Dilated/Atrous Convolution（空洞卷积），这种结构代替了池化，一方面它可以保持空间分辨率，另外一方面它由于可以扩大感受野因而可以很好地整合上下文信息（我觉得这个设计很有意思，原图的大小完全不会改变，也不需要上采样了）。</p> <p><img src="/assets/img/image-20210427221923919.cee6d0f9.png" alt="image-20210427221923919"></p> <p>上图：在某篇论文中出现的空洞卷积示意图。</p> <p><img src="/assets/img/Atrous_conv.e5e672a0.png" alt="Atrous_conv"></p> <p>上图：另一张空洞卷积的示意图。</p> <h3 id="条件随机场"><a href="#条件随机场" class="header-anchor">#</a> 条件随机场</h3> <p>在使用全卷积网络的分割方法中，有一个很常用的基本框架：</p> <p><img src="/assets/img/CRF01.520d4020.jpg" alt="img"></p> <p>其中， FCN 表示各种全卷积网络，CRF 为条件随机场，MRF 为马尔科夫随机场。其大致思路就是前端使用 FCN 进行特征粗提取，后端使用 CRF/MRF 优化前端的输出，最后得到分割图。</p> <p><a href="https://arxiv.org/pdf/1210.5644.pdf" target="_blank" rel="noopener noreferrer">条件随机场（Conditional Random Field，CRF）<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> 后处理操作通常用于进一步改善分割的效果。CRFs 是一种基于底层图像的像素强度进行“平滑”分割（‘smooth’ segmentation）的图模型，其工作原理是相似强度的像素更可能标记为同一类别。CRFs 一般能够提升 1-2% 的精度。</p> <p><img src="/assets/img/CRF.028b3443.png" alt="CRF"></p> <p>上图为CRF示意图。（b）一元分类结合CRF;（c, d, e）是CRF的变体，其中(e)是广泛使用的一种CRF。</p> <hr> <h2 id="分割的数据集"><a href="#分割的数据集" class="header-anchor">#</a> 分割的数据集</h2> <p>截止到原综述写作时间为止时较为流行的数据集：</p> <p><img src="/assets/img/image-20210428094548476.33b9fb04.png" alt="image-20210428094548476"></p> <p>还没看完，看完就写。</p> <hr> <h2 id="领域知名论文"><a href="#领域知名论文" class="header-anchor">#</a> 领域知名论文</h2> <h3 id="基于深度学习的分割方法"><a href="#基于深度学习的分割方法" class="header-anchor">#</a> 基于深度学习的分割方法</h3> <p><img src="/assets/img/image-20210428094705161.fa35d240.png" alt="image-20210428094705161"></p> <ol><li><p>FCN</p> <p>主要贡献：使端对端的卷积语义分割网络变得流行起来；通过deconvolutional layers进行上采样；通过skip connection改善了上采样的粗糙度。</p></li> <li><p>SegNet</p> <p>主要贡献：使用Maxpooling indices来增强位置信息。</p></li> <li><p>Dilated Convolutions</p> <p>主要贡献：使用空洞卷积用来进行稠密预测（dense prediction）；提出上下文模块（context module），使用空洞卷积（Dilated Convolutions）来进行多尺度信息的的整合。</p></li> <li><p>DeepLab (v1 &amp; v2)</p> <p>主要贡献：使用atrous卷积，也就是后来的空洞卷积，扩大感受野，保持分辨率；提出了atrous spatial pyramid pooling (ASPP)，整合多尺度信息；使用全连接条件随机场（fully connected CRF)进行后处理，改善分割结果。</p></li> <li><p>RefineNet</p> <p>主要贡献：精心设计了encoder-decoder架构中的decoder部分，使得性能提升；整个网络的设计都遵循residual connections，网络表达能力更强，梯度更容易反向传播。</p></li> <li><p>PSPNet</p> <p>主要贡献：使用pyramid pooling整合context；使用auxiliary loss。</p></li> <li><p>Large Kernel Matters</p> <p>主要贡献：提出一种具有非常大的内核卷积的编码器-解码器体系结构。</p></li> <li><p>DeepLab v3</p> <p>主要贡献：改进的无孔空间金字塔池化（ASPP）；级联使用atrous卷积的模块。</p></li></ol> <h3 id="上述方法的关系"><a href="#上述方法的关系" class="header-anchor">#</a> 上述方法的关系</h3> <p><img src="/assets/img/image-20210428094839526.90720117.png" alt="image-20210428094839526"></p></div> <footer class="page-edit"><!----> <div class="last-updated"><span class="prefix">上次更新时间:</span> <span class="time">2021/7/27 上午4:27:40</span></div></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/unlimited-paper-works/[9]Feature-Pyramid-Networks-for-Object-Detection.html" class="prev">
        Feature Pyramid Networks for Object Detection
      </a></span> <span class="next"><a href="/unlimited-paper-works/[11]Image-Segmentation-Using-Deep-Learning-A-Survey.html">
        Image Segmentation Using Deep Learning: A Survey
      </a>
      →
    </span></p></div> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.0cf0d483.js" defer></script><script src="/assets/js/2.d537feee.js" defer></script><script src="/assets/js/6.59e5684c.js" defer></script>
  </body>
</html>
